{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8370b6",
   "metadata": {},
   "source": [
    "# Length-Adaptive Sequential Recommendation - Paper Experiments\n",
    "\n",
    "**Publication-Quality Training on MovieLens-1M**\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Paper-Level Configuration\n",
    "\n",
    "**Training Settings:**\n",
    "- Max Epochs: 200 (with early stopping)\n",
    "- Patience: 20\n",
    "- Expected convergence: epoch 40-60\n",
    "- Batch size: 256\n",
    "- Learning rate: 0.001\n",
    "- Model: d_model=64, n_heads=2, n_blocks=2\n",
    "\n",
    "**Models to Train:**\n",
    "1. ‚úÖ SASRec (Transformer baseline)\n",
    "2. ‚úÖ Hybrid Fixed (Œ±=0.5)\n",
    "3. ‚úÖ Hybrid Discrete (bin-based fusion)\n",
    "4. ‚úÖ Hybrid Learnable (learned weights)\n",
    "5. ‚úÖ Hybrid Continuous (neural fusion)\n",
    "\n",
    "**Time Estimate: ~3-4 hours total with GPU T4**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Quick Start\n",
    "\n",
    "1. Enable GPU T4 accelerator\n",
    "2. Enable Internet\n",
    "3. Run cells 1-7 sequentially\n",
    "4. Download results.zip at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7282e75",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/faroukq1/length-adaptive.git\n",
    "\n",
    "# Change to project directory\n",
    "%cd length-adaptive\n",
    "\n",
    "# Verify structure\n",
    "!ls -lh experiments/\n",
    "\n",
    "print(\"\\n‚úÖ Repository cloned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba75e8b4",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages quietly\n",
    "!pip install -q torch-geometric tqdm scikit-learn pandas matplotlib\n",
    "\n",
    "print(\"‚úì All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d520f4",
   "metadata": {},
   "source": [
    "## Step 3: Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd253ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!python check_gpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d528121",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Data\n",
    "\n",
    "Downloads MovieLens-1M and preprocesses if needed (2-3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6986cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if preprocessed data exists\n",
    "data_file = 'data/ml-1m/processed/sequences.pkl'\n",
    "graph_file = 'data/graphs/cooccurrence_graph.pkl'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç Checking Data Files\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if os.path.exists(data_file):\n",
    "    print(f\"‚úÖ Sequential data found: {data_file}\")\n",
    "    print(f\"   Size: {os.path.getsize(data_file) / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Sequential data NOT found: {data_file}\")\n",
    "\n",
    "if os.path.exists(graph_file):\n",
    "    print(f\"‚úÖ Graph data found: {graph_file}\")\n",
    "    print(f\"   Size: {os.path.getsize(graph_file) / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Graph data NOT found: {graph_file}\")\n",
    "\n",
    "# Check raw data\n",
    "raw_file = 'data/ml-1m/raw/ml-1m/ratings.dat'\n",
    "if os.path.exists(raw_file):\n",
    "    print(f\"‚úÖ Raw data found: {raw_file}\")\n",
    "else:\n",
    "    print(f\"‚ùå Raw data NOT found: {raw_file}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# If data is missing, run preprocessing\n",
    "if not os.path.exists(data_file) or not os.path.exists(graph_file):\n",
    "    print(\"\\nüîß Running preprocessing...\")\n",
    "    print(\"This will take 2-3 minutes.\\n\")\n",
    "    \n",
    "    # Download MovieLens-1M if needed\n",
    "    if not os.path.exists(raw_file):\n",
    "        print(\"üì• Downloading MovieLens-1M dataset...\")\n",
    "        !mkdir -p data/ml-1m/raw\n",
    "        !wget -q http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "        !unzip -q ml-1m.zip\n",
    "        !mv ml-1m data/ml-1m/raw/\n",
    "        !rm -f ml-1m.zip\n",
    "        print(\"‚úÖ Download complete!\\n\")\n",
    "    \n",
    "    # Run preprocessing\n",
    "    print(\"üîÑ Preprocessing sequential data...\")\n",
    "    !python -m src.data.preprocess\n",
    "    \n",
    "    # Build graph\n",
    "    print(\"\\nüîÑ Building co-occurrence graph...\")\n",
    "    !python -m src.data.graph_builder\n",
    "    \n",
    "    print(\"\\n‚úÖ Preprocessing complete!\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n‚úÖ All data files ready!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0afc0d",
   "metadata": {},
   "source": [
    "## Step 5: Run Paper Experiments\n",
    "\n",
    "**‚è±Ô∏è Time: ~3-4 hours total (GPU T4)**\n",
    "\n",
    "This will train all 5 models sequentially with paper-quality settings:\n",
    "- 200 max epochs with early stopping (patience=20)\n",
    "- Models typically converge at epoch 40-60\n",
    "- Full ablation study for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all paper experiments\n",
    "print(\"=\"*80)\n",
    "print(\"üéì PAPER-LEVEL EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"Training 5 models with 200 epochs, early stopping patience=20\")\n",
    "print(\"Expected convergence: epoch 40-60\")\n",
    "print(\"Time estimate: ~3-4 hours with GPU T4\")\n",
    "print(\"\")\n",
    "print(\"Models:\")\n",
    "print(\"  1. SASRec (baseline)\")\n",
    "print(\"  2. Hybrid Fixed (Œ±=0.5)\")\n",
    "print(\"  3. Hybrid Discrete (bin-based)\")\n",
    "print(\"  4. Hybrid Learnable (learned weights)\")\n",
    "print(\"  5. Hybrid Continuous (neural fusion)\")\n",
    "print(\"\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run the paper experiments script\n",
    "!bash scripts/run_paper_experiments.sh\n",
    "\n",
    "print(\"\\n‚úÖ All paper experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ab7cc",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Results\n",
    "\n",
    "Generate comprehensive comparison tables and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate analysis\n",
    "print(\"=\"*70)\n",
    "print(\"üìä Generating Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!python experiments/analyze_results.py --save_csv\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f526b9",
   "metadata": {},
   "source": [
    "## Step 7: Display Results\n",
    "\n",
    "Show comprehensive performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef63b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Try to load results directly from experiments\n",
    "result_folders = glob.glob('results/*_*')\n",
    "\n",
    "if len(result_folders) == 0:\n",
    "    print(\"‚ùå No results found. Run experiments first!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä OVERALL PERFORMANCE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Collect all results\n",
    "    all_results = []\n",
    "    for folder in result_folders:\n",
    "        results_path = os.path.join(folder, 'results.json')\n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Extract model name\n",
    "            folder_name = os.path.basename(folder)\n",
    "            model_name = '_'.join(folder_name.split('_')[:-2])\n",
    "            \n",
    "            all_results.append({\n",
    "                'Model': model_name,\n",
    "                'HR@5': results['test_metrics']['HR@5'],\n",
    "                'HR@10': results['test_metrics']['HR@10'],\n",
    "                'HR@20': results['test_metrics']['HR@20'],\n",
    "                'NDCG@5': results['test_metrics']['NDCG@5'],\n",
    "                'NDCG@10': results['test_metrics']['NDCG@10'],\n",
    "                'NDCG@20': results['test_metrics']['NDCG@20'],\n",
    "                'MRR@10': results['test_metrics']['MRR@10']\n",
    "            })\n",
    "    \n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        df = df.sort_values('NDCG@10', ascending=False)\n",
    "        \n",
    "        # Display table\n",
    "        print(df.to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        # Highlight best model\n",
    "        best = df.iloc[0]\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"üèÜ BEST MODEL: {best['Model']}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"  NDCG@10: {best['NDCG@10']:.4f}\")\n",
    "        print(f\"  HR@10:   {best['HR@10']:.4f}\")\n",
    "        print(f\"  MRR@10:  {best['MRR@10']:.4f}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Show improvement over baseline\n",
    "        sasrec_row = df[df['Model'] == 'sasrec']\n",
    "        if not sasrec_row.empty:\n",
    "            sasrec_ndcg = sasrec_row.iloc[0]['NDCG@10']\n",
    "            sasrec_hr = sasrec_row.iloc[0]['HR@10']\n",
    "            hybrid_ndcg = best['NDCG@10']\n",
    "            hybrid_hr = best['HR@10']\n",
    "            ndcg_imp = ((hybrid_ndcg - sasrec_ndcg) / sasrec_ndcg) * 100\n",
    "            hr_imp = ((hybrid_hr - sasrec_hr) / sasrec_hr) * 100\n",
    "            print(f\"üìà Improvement over SASRec baseline:\")\n",
    "            print(f\"   NDCG@10: {ndcg_imp:+.2f}%\")\n",
    "            print(f\"   HR@10:   {hr_imp:+.2f}%\\n\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not parse results files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d125be1",
   "metadata": {},
   "source": [
    "## Step 8: Performance by User Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471acda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load grouped metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä PERFORMANCE BY USER GROUP\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "result_folders = glob.glob('results/*_*')\n",
    "\n",
    "if len(result_folders) == 0:\n",
    "    print(\"‚ùå No results found.\")\n",
    "else:\n",
    "    # Collect grouped results\n",
    "    group_data = {'short': [], 'medium': [], 'long': []}\n",
    "    \n",
    "    for folder in result_folders:\n",
    "        results_path = os.path.join(folder, 'results.json')\n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Extract model name\n",
    "            folder_name = os.path.basename(folder)\n",
    "            model_name = '_'.join(folder_name.split('_')[:-2])\n",
    "            \n",
    "            # Extract grouped metrics\n",
    "            grouped = results.get('grouped_metrics', {})\n",
    "            \n",
    "            for group in ['short', 'medium', 'long']:\n",
    "                if group in grouped:\n",
    "                    group_data[group].append({\n",
    "                        'Model': model_name,\n",
    "                        'HR@10': grouped[group]['HR@10'],\n",
    "                        'NDCG@10': grouped[group]['NDCG@10'],\n",
    "                        'MRR@10': grouped[group]['MRR@10'],\n",
    "                        'Count': grouped[group]['count']\n",
    "                    })\n",
    "    \n",
    "    # Display each group\n",
    "    for group_name in ['short', 'medium', 'long']:\n",
    "        if group_data[group_name]:\n",
    "            df_group = pd.DataFrame(group_data[group_name])\n",
    "            df_group = df_group.sort_values('NDCG@10', ascending=False)\n",
    "            \n",
    "            print(f\"\\n{group_name.upper()} HISTORY USERS:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(df_group.to_string(index=False, float_format='%.4f'))\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"\\n{group_name.upper()} HISTORY USERS:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"‚ö†Ô∏è  No {group_name} user data found (possibly no users in this range)\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc3eb91",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Find all experiment results\n",
    "result_folders = glob.glob('results/*_*')\n",
    "\n",
    "if len(result_folders) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    for folder in result_folders:\n",
    "        history_path = os.path.join(folder, 'history.json')\n",
    "        if os.path.exists(history_path):\n",
    "            try:\n",
    "                with open(history_path, 'r') as f:\n",
    "                    history = json.load(f)\n",
    "                \n",
    "                # Extract model name from folder\n",
    "                parts = os.path.basename(folder).split('_')\n",
    "                model_name = '_'.join(parts[:-2]) if len(parts) > 2 else parts[0]\n",
    "                \n",
    "                if 'train_loss' in history and history['train_loss']:\n",
    "                    ax1.plot(history['train_loss'], label=model_name, marker='o', markersize=3, linewidth=2)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not load history from {folder}: {e}\")\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('BPR Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Validation NDCG@10\n",
    "    for folder in result_folders:\n",
    "        history_path = os.path.join(folder, 'history.json')\n",
    "        if os.path.exists(history_path):\n",
    "            try:\n",
    "                with open(history_path, 'r') as f:\n",
    "                    history = json.load(f)\n",
    "                \n",
    "                parts = os.path.basename(folder).split('_')\n",
    "                model_name = '_'.join(parts[:-2]) if len(parts) > 2 else parts[0]\n",
    "                \n",
    "                if 'val_metrics' in history and history['val_metrics']:\n",
    "                    ndcg_values = [m.get('NDCG@10', 0) for m in history['val_metrics']]\n",
    "                    if ndcg_values:\n",
    "                        ax2.plot(ndcg_values, label=model_name, marker='o', markersize=3, linewidth=2)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not load validation metrics from {folder}: {e}\")\n",
    "    \n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('NDCG@10', fontsize=12)\n",
    "    ax2.set_title('Validation NDCG@10', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    plt.savefig('results/learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Saved to: results/learning_curves.png\")\n",
    "else:\n",
    "    print(\"No results to plot. Run experiments first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27fb6e",
   "metadata": {},
   "source": [
    "## Step 10: Download Results\n",
    "\n",
    "Creates a zip file with all results for local analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip of all results\n",
    "import os\n",
    "\n",
    "if os.path.exists('results') and os.listdir('results'):\n",
    "    !zip -r results_paper.zip results/\n",
    "    \n",
    "    print(\"\\n‚úÖ Success!\")\n",
    "    print(\"Download 'results_paper.zip' from the Output tab (right sidebar) ‚Üí\")\n",
    "    print(\"\\nContains:\")\n",
    "    print(\"  ‚Ä¢ Model checkpoints (best_model.pt)\")\n",
    "    print(\"  ‚Ä¢ Training history (history.json)\")\n",
    "    print(\"  ‚Ä¢ Test metrics (results.json)\")\n",
    "    print(\"  ‚Ä¢ Comparison tables (CSV files)\")\n",
    "    print(\"  ‚Ä¢ Learning curves (PNG)\")\n",
    "    \n",
    "    # Show what's in results\n",
    "    result_folders = [d for d in os.listdir('results') if os.path.isdir(os.path.join('results', d))]\n",
    "    print(f\"\\nüì¶ Packaged {len(result_folders)} experiment(s):\")\n",
    "    for folder in result_folders:\n",
    "        print(f\"  ‚Ä¢ {folder}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results folder found. Run experiments first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cef58e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Paper Experiments Complete!\n",
    "\n",
    "You now have publication-quality results for:\n",
    "- SASRec baseline\n",
    "- 4 hybrid fusion strategies\n",
    "- Complete ablation study\n",
    "- Performance by user groups\n",
    "- Learning curves visualization\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download `results_paper.zip`\n",
    "2. Use for paper tables and figures\n",
    "3. Report best model and improvements over baseline\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Citation\n",
    "\n",
    "```\n",
    "@article{yourname2026length,\n",
    "  title={Length-Adaptive Hybrid Sequential Recommendation},\n",
    "  author={Your Name},\n",
    "  journal={arXiv preprint},\n",
    "  year={2026}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
