# Running Experiments Guide

This guide explains how to run experiments and analyze results.

## Quick Start

### 1. Test the Training Pipeline (2 epochs)

First, verify everything works with a quick test:

```bash
python test_training.py
```

This will:

- Train SASRec for 2 epochs
- Train Hybrid model for 2 epochs
- Test evaluation metrics
- Verify checkpoint saving/loading

**Expected time**: ~5-10 minutes

---

### 2. Run a Single Experiment (50 epochs)

To train a specific model:

```bash
# Train SASRec baseline
python experiments/run_experiment.py --model sasrec --epochs 50

# Train Hybrid with discrete fusion (recommended)
python experiments/run_experiment.py --model hybrid_discrete --epochs 50

# Train Hybrid with learnable fusion
python experiments/run_experiment.py --model hybrid_learnable --epochs 50

# Train Hybrid with continuous fusion
python experiments/run_experiment.py --model hybrid_continuous --epochs 50

# Train Hybrid with fixed fusion
python experiments/run_experiment.py --model hybrid_fixed --epochs 50 --fixed_alpha 0.5
```

**Expected time per model**: ~30-60 minutes (CPU)

---

### 3. Run All Experiments (Batch)

To train all models and compare results:

```bash
bash scripts/run_all_experiments.sh
```

This will train all 5 model variants sequentially.

**Expected time**: ~3-5 hours (CPU)

---

### 4. Analyze Results

After experiments complete, analyze and compare results:

```bash
# Print comparison table
python experiments/analyze_results.py

# Save comparison as CSV files
python experiments/analyze_results.py --save_csv
```

This will show:

- Overall performance comparison
- Performance by user group (short/medium/long history)
- Best model per group
- Improvements over SASRec baseline

---

## Command Line Options

### Model Selection

```bash
--model {sasrec, hybrid_fixed, hybrid_discrete, hybrid_learnable, hybrid_continuous}
```

### Training Parameters

```bash
--epochs 50              # Number of training epochs
--batch_size 256         # Batch size
--lr 0.001              # Learning rate
--patience 10           # Early stopping patience
--eval_every 1          # Evaluate every N epochs
```

### Model Architecture

```bash
--d_model 64            # Embedding dimension
--n_heads 2             # Number of attention heads
--n_blocks 2            # Number of transformer blocks
--d_ff 256              # Feed-forward dimension
--gnn_layers 2          # Number of GNN layers
--max_len 50            # Maximum sequence length
--dropout 0.2           # Dropout rate
```

### Fusion Parameters (Hybrid models only)

```bash
--L_short 10            # Short history threshold
--L_long 50             # Long history threshold
--fixed_alpha 0.5       # Fixed alpha (for hybrid_fixed)
```

---

## Example: Custom Experiment

```bash
python experiments/run_experiment.py \
    --model hybrid_continuous \
    --epochs 100 \
    --batch_size 512 \
    --lr 0.0005 \
    --d_model 128 \
    --n_heads 4 \
    --n_blocks 3 \
    --patience 15 \
    --save_dir my_experiments
```

---

## Output Structure

Each experiment creates a timestamped folder in `results/`:

```
results/
├── hybrid_discrete_20240101_120000/
│   ├── config.json          # Experiment configuration
│   ├── history.json         # Training history
│   ├── results.json         # Test metrics
│   ├── best_model.pt        # Best model checkpoint
│   └── checkpoint_epoch_10.pt
├── sasrec_20240101_130000/
│   └── ...
└── overall_comparison.csv   # Generated by analyze_results.py
```

---

## Metrics

All experiments report:

- **HR@K** (Hit Rate): Recall at top-K
- **NDCG@K** (Normalized DCG): Ranking quality
- **MRR@K** (Mean Reciprocal Rank): Average rank

For K = {5, 10, 20}

Plus **grouped metrics** by sequence length:

- **short**: 1-10 items
- **medium**: 11-50 items
- **long**: 51+ items

---

## GPU Acceleration

If you have a GPU with CUDA:

```bash
# GPU will be used automatically
python experiments/run_experiment.py --model hybrid_discrete --epochs 50

# Force CPU (for testing)
python experiments/run_experiment.py --model hybrid_discrete --epochs 50 --cpu
```

---

## Tips

1. **Start small**: Run `test_training.py` first to verify setup
2. **Test one model**: Train one model for 10-20 epochs to check convergence
3. **Monitor validation**: Watch for overfitting (val metric decreasing)
4. **Early stopping**: Enabled by default, saves best model automatically
5. **Compare fairly**: Use same hyperparameters across all models

---

## Troubleshooting

### OOM Errors

```bash
# Reduce batch size
--batch_size 128

# Reduce model size
--d_model 32 --n_heads 1 --n_blocks 1
```

### Slow Training

```bash
# Reduce epochs
--epochs 20

# Reduce data workers
--num_workers 2

# Use smaller sequences
--max_len 30
```

### Poor Performance

```bash
# Try different learning rate
--lr 0.0001  # Lower
--lr 0.01    # Higher

# Increase model capacity
--d_model 128 --n_blocks 3

# Train longer
--epochs 100 --patience 20
```

---

## Expected Results

Based on MovieLens-1M:

| Model               | NDCG@10   | HR@10     | Note              |
| ------------------- | --------- | --------- | ----------------- |
| SASRec              | ~0.15     | ~0.25     | Baseline          |
| Hybrid (Fixed)      | ~0.16     | ~0.27     | Simple fusion     |
| Hybrid (Discrete)   | ~0.17     | ~0.28     | Length-aware bins |
| Hybrid (Learnable)  | ~0.17     | ~0.29     | Learned weights   |
| Hybrid (Continuous) | **~0.18** | **~0.30** | Best overall      |

**Note**: Actual results may vary based on hyperparameters and random seed.

---

## Next Steps

After running experiments:

1. Analyze results with `analyze_results.py`
2. Identify best model variant
3. Test with different hyperparameters
4. Run ablation studies
5. Write up findings in paper/report
