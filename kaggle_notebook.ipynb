{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f998c25",
   "metadata": {},
   "source": [
    "# Length-Adaptive Sequential Recommendation\n",
    "\n",
    "**Hybrid SASRec + LightGCN with Adaptive Fusion on MovieLens-1M**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "This notebook is ready to run! Just:\n",
    "1. **Enable GPU** (recommended): Settings ‚Üí Accelerator ‚Üí GPU T4\n",
    "2. **Click \"Run All\"** or run cells sequentially\n",
    "\n",
    "**Expected Time:**\n",
    "- With GPU T4: ~8-10 minutes per model\n",
    "- With CPU: ~40-50 minutes per model\n",
    "\n",
    "All data is already preprocessed and included in the repository!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4a69f",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository\n",
    "\n",
    "Cloning from: https://github.com/faroukq1/length-adaptive.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2293cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository with all preprocessed data\n",
    "!git clone https://github.com/faroukq1/length-adaptive.git\n",
    "\n",
    "# Change to project directory\n",
    "%cd length-adaptive\n",
    "\n",
    "# Verify structure\n",
    "!echo \"‚úì Source code:\"\n",
    "!ls -la src/\n",
    "\n",
    "!echo \"\\n‚úì Preprocessed data:\"\n",
    "!ls -lh data/ml-1m/processed/\n",
    "\n",
    "!echo \"\\n‚úì Co-occurrence graph:\"\n",
    "!ls -lh data/graphs/\n",
    "\n",
    "!echo \"\\n‚úì Experiments scripts:\"\n",
    "!ls -lh experiments/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb798d",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "Installing PyTorch Geometric and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages quietly\n",
    "!pip install -q torch-geometric tqdm scikit-learn pandas matplotlib\n",
    "\n",
    "print(\"‚úì All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143777ec",
   "metadata": {},
   "source": [
    "## Step 3: Verify GPU Setup\n",
    "\n",
    "Check if GPU is available and will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!python check_gpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ee136",
   "metadata": {},
   "source": [
    "## üìã Experiment Priority Guide\n",
    "\n",
    "This notebook includes experiments from the action plan to beat SASRec baseline:\n",
    "\n",
    "**Priority 1 (Quick - Run First):**\n",
    "- ‚úÖ SASRec Baseline (Step 6)\n",
    "- ‚úÖ Hybrid Discrete (Step 5) - Our best model\n",
    "\n",
    "**Priority 2 (Optimization - Run if time permits):**\n",
    "- üî¨ Grid Search for Optimal Alpha (Advanced section)\n",
    "- üî¨ All Hybrid Variants (Advanced section)\n",
    "\n",
    "**Current Best Results:**\n",
    "- Hybrid Fixed (Œ±=0.5): HR@10 = 9.99% (+3.7% vs baseline)\n",
    "- Short-history users: +42% improvement\n",
    "\n",
    "**Target:** Beat SASRec on overall HR@10 by ‚â•3% and short-user HR@10 by ‚â•20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225334f",
   "metadata": {},
   "source": [
    "## Step 4: Quick Test (2 epochs)\n",
    "\n",
    "Verify everything works with a quick 2-epoch test on both SASRec and Hybrid models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick training test\n",
    "!python test_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea70fe",
   "metadata": {},
   "source": [
    "## Step 5: Train Hybrid Model (50 epochs)\n",
    "\n",
    "Train our best model: **Hybrid with Discrete Fusion**\n",
    "\n",
    "This model adapts based on user history length:\n",
    "- Short history users (‚â§10 items): More collaborative filtering (GNN)\n",
    "- Medium users (10-50 items): Balanced fusion\n",
    "- Long history users (>50 items): More sequential patterns (Transformer)\n",
    "\n",
    "**Note on Epochs:**\n",
    "- **50 epochs** = ~8-10 min (GPU) with early stopping ‚Üí typically converges at ~20-30 epochs\n",
    "- **600 epochs** (paper setting) = ~80 min (GPU) ‚Üí may improve by ~2-3% but uses 10x time\n",
    "- Early stopping (patience=10) prevents overfitting automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Hybrid model with discrete fusion (default: Œ±_short=0.3, Œ±_mid=0.5, Œ±_long=0.7)\n",
    "# This uses early stopping - training stops when validation stops improving\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ Training Hybrid Discrete Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!python experiments/run_experiment.py \\\n",
    "    --model hybrid_discrete \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.001 \\\n",
    "    --d_model 64 \\\n",
    "    --n_heads 2 \\\n",
    "    --n_blocks 2 \\\n",
    "    --patience 10\n",
    "\n",
    "print(\"\\n‚úÖ Training complete! Check results/ folder for outputs.\")\n",
    "\n",
    "# To match paper settings (slower but may get slightly better results):\n",
    "# !python experiments/run_experiment.py \\\n",
    "#     --model hybrid_discrete \\\n",
    "#     --epochs 600 \\\n",
    "#     --patience 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54faf4d7",
   "metadata": {},
   "source": [
    "## Step 6: Train SASRec Baseline (Optional - Skip if you already have it)\n",
    "\n",
    "**‚ö†Ô∏è SKIP THIS STEP if:**\n",
    "- You already have `results/sasrec_*/` folder from previous runs\n",
    "- You haven't changed data preprocessing or hyperparameters\n",
    "- You just want to test new hybrid variants\n",
    "\n",
    "**Only run this if:**\n",
    "- First time training\n",
    "- Changed hyperparameters\n",
    "- Want to verify reproducibility\n",
    "- Need fresh baseline for comparison\n",
    "\n",
    "**Alternative:** Copy your existing `results/sasrec_*/` folder to Kaggle instead of retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Skip SASRec training (if you already have results)\n",
    "print(\"üí° Skipping SASRec - using existing baseline results\")\n",
    "print(\"   If you need to train SASRec, uncomment the code below:\\n\")\n",
    "\n",
    "# OPTION 2: Train SASRec baseline (uncomment if needed)\n",
    "# print(\"=\"*70)\n",
    "# print(\"üöÄ Training SASRec Baseline\")\n",
    "# print(\"=\"*70)\n",
    "# \n",
    "# !python experiments/run_experiment.py \\\n",
    "#     --model sasrec \\\n",
    "#     --epochs 50 \\\n",
    "#     --batch_size 256 \\\n",
    "#     --lr 0.001 \\\n",
    "#     --patience 10\n",
    "# \n",
    "# print(\"\\n‚úÖ Baseline training complete!\")\n",
    "\n",
    "# OPTION 3: Upload existing SASRec results\n",
    "# If you have results locally, you can upload the folder:\n",
    "# 1. Zip your local results/sasrec_*/ folder\n",
    "# 2. Upload to Kaggle input data\n",
    "# 3. Copy to results/ directory:\n",
    "# !mkdir -p results\n",
    "# !cp -r /kaggle/input/your-sasrec-results/* results/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab458a",
   "metadata": {},
   "source": [
    "## Step 7: Train All Models (Optional - takes 3-5 hours)\n",
    "\n",
    "Uncomment to train all 5 model variants:\n",
    "- `sasrec`: Transformer baseline\n",
    "- `hybrid_fixed`: Fixed fusion weight (Œ±=0.5)\n",
    "- `hybrid_discrete`: Bin-based fusion (our approach)\n",
    "- `hybrid_learnable`: Per-user learned weights\n",
    "- `hybrid_continuous`: Neural network fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run all experiments\n",
    "# !bash scripts/run_all_experiments.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5817e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all hybrid variants\n",
    "# Uncomment to run complete ablation study (takes ~8 hours with GPU)\n",
    "\n",
    "# models = ['hybrid_fixed', 'hybrid_discrete', 'hybrid_learnable', 'hybrid_continuous']\n",
    "# \n",
    "# for model in models:\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"üöÄ Training {model}\")\n",
    "#     print(f\"{'='*70}\\n\")\n",
    "#     \n",
    "#     !python experiments/run_experiment.py \\\n",
    "#         --model {model} \\\n",
    "#         --epochs 50 \\\n",
    "#         --batch_size 256 \\\n",
    "#         --lr 0.001 \\\n",
    "#         --patience 10\n",
    "#     \n",
    "#     print(f\"\\n‚úÖ {model} complete!\")\n",
    "\n",
    "# Quick version: Use the automated script\n",
    "# !bash scripts/run_all_experiments.sh\n",
    "\n",
    "print(\"üí° Tip: Uncomment to train all model variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0529e222",
   "metadata": {},
   "source": [
    "## üî¨ Advanced: All Hybrid Variants\n",
    "\n",
    "Train all fusion strategies for complete comparison:\n",
    "- **Fixed**: Single Œ± for all users\n",
    "- **Discrete**: Bin-based (short/medium/long)\n",
    "- **Learnable**: Learned bin weights\n",
    "- **Continuous**: Smooth function of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62309b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for optimal alpha\n",
    "# Tests Œ± ‚àà {0.3, 0.4, 0.5, 0.6, 0.7}\n",
    "# Uncomment to run (takes ~10-12 hours with GPU)\n",
    "\n",
    "# alphas = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "# \n",
    "# for alpha in alphas:\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"üî¨ Testing Fixed Alpha = {alpha}\")\n",
    "#     print(f\"{'='*70}\\n\")\n",
    "#     \n",
    "#     !python experiments/run_experiment.py \\\n",
    "#         --model hybrid_fixed \\\n",
    "#         --fixed_alpha {alpha} \\\n",
    "#         --epochs 50 \\\n",
    "#         --batch_size 256 \\\n",
    "#         --lr 0.001 \\\n",
    "#         --patience 10\n",
    "#     \n",
    "#     print(f\"\\n‚úÖ Alpha={alpha} complete!\")\n",
    "\n",
    "print(\"üí° Tip: Uncomment the code above to run grid search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2209e8",
   "metadata": {},
   "source": [
    "## üî¨ Advanced: Grid Search for Optimal Alpha (Fixed Fusion)\n",
    "\n",
    "Test different fixed alpha values to find the optimal fusion weight.\n",
    "This helps us understand the best balance between GNN and SASRec embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06b542",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Results\n",
    "\n",
    "Generate comparison tables and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af94e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate analysis using the built-in script\n",
    "print(\"=\"*70)\n",
    "print(\"üìä Generating Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!python experiments/analyze_results.py --save_csv\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17afa32c",
   "metadata": {},
   "source": [
    "## Step 9: Display Results\n",
    "\n",
    "Show performance comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f509c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Try to load results directly from experiments\n",
    "result_folders = glob.glob('results/*_*')\n",
    "\n",
    "if len(result_folders) == 0:\n",
    "    print(\"‚ùå No results found. Run experiments first!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä OVERALL PERFORMANCE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Collect all results\n",
    "    all_results = []\n",
    "    for folder in result_folders:\n",
    "        results_path = os.path.join(folder, 'results.json')\n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Extract model name\n",
    "            folder_name = os.path.basename(folder)\n",
    "            model_name = '_'.join(folder_name.split('_')[:-2])\n",
    "            \n",
    "            all_results.append({\n",
    "                'Model': model_name,\n",
    "                'HR@5': results['test_metrics']['HR@5'],\n",
    "                'HR@10': results['test_metrics']['HR@10'],\n",
    "                'HR@20': results['test_metrics']['HR@20'],\n",
    "                'NDCG@5': results['test_metrics']['NDCG@5'],\n",
    "                'NDCG@10': results['test_metrics']['NDCG@10'],\n",
    "                'NDCG@20': results['test_metrics']['NDCG@20'],\n",
    "                'MRR@10': results['test_metrics']['MRR@10']\n",
    "            })\n",
    "    \n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        df = df.sort_values('NDCG@10', ascending=False)\n",
    "        \n",
    "        # Display table\n",
    "        print(df.to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        # Highlight best model\n",
    "        best = df.iloc[0]\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"üèÜ BEST MODEL: {best['Model']}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"  NDCG@10: {best['NDCG@10']:.4f}\")\n",
    "        print(f\"  HR@10:   {best['HR@10']:.4f}\")\n",
    "        print(f\"  MRR@10:  {best['MRR@10']:.4f}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Show improvement over baseline\n",
    "        sasrec_row = df[df['Model'] == 'sasrec']\n",
    "        if not sasrec_row.empty:\n",
    "            sasrec_ndcg = sasrec_row.iloc[0]['NDCG@10']\n",
    "            sasrec_hr = sasrec_row.iloc[0]['HR@10']\n",
    "            hybrid_ndcg = best['NDCG@10']\n",
    "            hybrid_hr = best['HR@10']\n",
    "            ndcg_imp = ((hybrid_ndcg - sasrec_ndcg) / sasrec_ndcg) * 100\n",
    "            hr_imp = ((hybrid_hr - sasrec_hr) / sasrec_hr) * 100\n",
    "            print(f\"üìà Improvement over SASRec baseline:\")\n",
    "            print(f\"   NDCG@10: {ndcg_imp:+.2f}%\")\n",
    "            print(f\"   HR@10:   {hr_imp:+.2f}%\\n\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not parse results files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ae64e",
   "metadata": {},
   "source": [
    "## Step 10: Performance by User Group\n",
    "\n",
    "Compare performance across different user history lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc663df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load grouped metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä PERFORMANCE BY USER GROUP\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "result_folders = glob.glob('results/*_*')\n",
    "\n",
    "if len(result_folders) == 0:\n",
    "    print(\"‚ùå No results found.\")\n",
    "else:\n",
    "    # Collect grouped results\n",
    "    group_data = {'short': [], 'medium': [], 'long': []}\n",
    "    \n",
    "    for folder in result_folders:\n",
    "        results_path = os.path.join(folder, 'results.json')\n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Extract model name\n",
    "            folder_name = os.path.basename(folder)\n",
    "            model_name = '_'.join(folder_name.split('_')[:-2])\n",
    "            \n",
    "            # Extract grouped metrics\n",
    "            grouped = results.get('grouped_metrics', {})\n",
    "            \n",
    "            for group in ['short', 'medium', 'long']:\n",
    "                if group in grouped:\n",
    "                    group_data[group].append({\n",
    "                        'Model': model_name,\n",
    "                        'HR@10': grouped[group]['HR@10'],\n",
    "                        'NDCG@10': grouped[group]['NDCG@10'],\n",
    "                        'MRR@10': grouped[group]['MRR@10'],\n",
    "                        'Count': grouped[group]['count']\n",
    "                    })\n",
    "    \n",
    "    # Display each group\n",
    "    for group_name in ['short', 'medium', 'long']:\n",
    "        if group_data[group_name]:\n",
    "            df_group = pd.DataFrame(group_data[group_name])\n",
    "            df_group = df_group.sort_values('NDCG@10', ascending=False)\n",
    "            \n",
    "            print(f\"\\n{group_name.upper()} HISTORY USERS:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(df_group.to_string(index=False, float_format='%.4f'))\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"\\n{group_name.upper()} HISTORY USERS:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"‚ö†Ô∏è  No {group_name} user data found (possibly no users in this range)\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Check for alpha statistics in hybrid model results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç ALPHA VALUES (Fusion Weights)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "hybrid_folders = [f for f in glob.glob('results/hybrid_*') if os.path.isdir(f)]\n",
    "\n",
    "if not hybrid_folders:\n",
    "    print(\"‚ö†Ô∏è  No hybrid model results found. Alpha tracking only works for hybrid models.\")\n",
    "else:\n",
    "    for folder in hybrid_folders:\n",
    "        alpha_path = os.path.join(folder, 'alpha_stats.json')\n",
    "        if os.path.exists(alpha_path):\n",
    "            with open(alpha_path, 'r') as f:\n",
    "                alpha_stats = json.load(f)\n",
    "            \n",
    "            folder_name = os.path.basename(folder)\n",
    "            model_name = '_'.join(folder_name.split('_')[:-2])\n",
    "            \n",
    "            print(f\"{model_name.upper()}:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for group in ['short', 'medium', 'long', 'overall']:\n",
    "                if group in alpha_stats:\n",
    "                    stats = alpha_stats[group]\n",
    "                    if group != 'overall' and 'count' in stats:\n",
    "                        print(f\"  {group.capitalize():8s}: mean={stats['mean']:.3f}, std={stats['std']:.3f}, count={stats['count']}\")\n",
    "                    elif group == 'overall':\n",
    "                        print(f\"  {group.capitalize():8s}: mean={stats['mean']:.3f}, std={stats['std']:.3f}\")\n",
    "            print()\n",
    "        else:\n",
    "            # Show expected alpha values based on model type\n",
    "            folder_name = os.path.basename(folder)\n",
    "            model_name = '_'.join(folder_name.split('_')[:-2])\n",
    "            \n",
    "            print(f\"{model_name.upper()}:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            if 'discrete' in model_name:\n",
    "                print(\"  Expected: Short=0.3, Medium=0.5, Long=0.7 (discrete bins)\")\n",
    "            elif 'fixed' in model_name:\n",
    "                print(\"  Expected: All users = 0.5 (fixed fusion)\")\n",
    "            elif 'learnable' in model_name:\n",
    "                print(\"  Expected: Learned during training (check model params)\")\n",
    "            elif 'continuous' in model_name:\n",
    "                print(\"  Expected: Smooth function of sequence length\")\n",
    "            \n",
    "            print(\"  ‚ö†Ô∏è  Alpha statistics not saved (enable with track_alpha=True)\")\n",
    "            print()\n",
    "    \n",
    "    print(\"\\nüí° Alpha interpretation:\")\n",
    "    print(\"   ‚Ä¢ Œ± close to 0: More weight on GNN (collaborative)\")\n",
    "    print(\"   ‚Ä¢ Œ± close to 1: More weight on SASRec (sequential)\")\n",
    "    print(\"   ‚Ä¢ Œ± = 0.5: Equal balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086cb30",
   "metadata": {},
   "source": [
    "## Step 10b: Alpha Statistics (Hybrid Models Only)\n",
    "\n",
    "For hybrid models, check what fusion weights (alpha values) were used for different user groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2feda3",
   "metadata": {},
   "source": [
    "## Step 11: Visualize Learning Curves\n",
    "\n",
    "Plot training loss and validation NDCG over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Find all experiment results\n",
    "result_folders = glob.glob('results/*_*')\n",
    "\n",
    "if len(result_folders) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    for folder in result_folders:\n",
    "        history_path = os.path.join(folder, 'history.json')\n",
    "        if os.path.exists(history_path):\n",
    "            try:\n",
    "                with open(history_path, 'r') as f:\n",
    "                    history = json.load(f)\n",
    "                \n",
    "                # Extract model name from folder\n",
    "                parts = os.path.basename(folder).split('_')\n",
    "                model_name = '_'.join(parts[:-2]) if len(parts) > 2 else parts[0]\n",
    "                \n",
    "                if 'train_loss' in history and history['train_loss']:\n",
    "                    ax1.plot(history['train_loss'], label=model_name, marker='o', markersize=3, linewidth=2)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not load history from {folder}: {e}\")\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('BPR Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Validation NDCG@10\n",
    "    for folder in result_folders:\n",
    "        history_path = os.path.join(folder, 'history.json')\n",
    "        if os.path.exists(history_path):\n",
    "            try:\n",
    "                with open(history_path, 'r') as f:\n",
    "                    history = json.load(f)\n",
    "                \n",
    "                parts = os.path.basename(folder).split('_')\n",
    "                model_name = '_'.join(parts[:-2]) if len(parts) > 2 else parts[0]\n",
    "                \n",
    "                if 'val_metrics' in history and history['val_metrics']:\n",
    "                    ndcg_values = [m.get('NDCG@10', 0) for m in history['val_metrics']]\n",
    "                    if ndcg_values:\n",
    "                        ax2.plot(ndcg_values, label=model_name, marker='o', markersize=3, linewidth=2)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not load validation metrics from {folder}: {e}\")\n",
    "    \n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('NDCG@10', fontsize=12)\n",
    "    ax2.set_title('Validation NDCG@10', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    plt.savefig('results/learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Saved to: results/learning_curves.png\")\n",
    "else:\n",
    "    print(\"No results to plot. Run experiments first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1299e15",
   "metadata": {},
   "source": [
    "## Step 12: Download Results\n",
    "\n",
    "Create a zip file of all results for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip of all results\n",
    "import os\n",
    "\n",
    "if os.path.exists('results') and os.listdir('results'):\n",
    "    !zip -r results.zip results/\n",
    "    \n",
    "    print(\"\\n‚úÖ Success!\")\n",
    "    print(\"Download 'results.zip' from the Output tab (right sidebar) ‚Üí\")\n",
    "    print(\"\\nContains:\")\n",
    "    print(\"  ‚Ä¢ Model checkpoints (best_model.pt)\")\n",
    "    print(\"  ‚Ä¢ Training history (history.json)\")\n",
    "    print(\"  ‚Ä¢ Test metrics (results.json)\")\n",
    "    print(\"  ‚Ä¢ Comparison tables (CSV files, if generated)\")\n",
    "    print(\"  ‚Ä¢ Learning curves (PNG)\")\n",
    "    \n",
    "    # Show what's in results\n",
    "    result_folders = [d for d in os.listdir('results') if os.path.isdir(os.path.join('results', d))]\n",
    "    print(f\"\\nüì¶ Packaged {len(result_folders)} experiment(s):\")\n",
    "    for folder in result_folders:\n",
    "        print(f\"  ‚Ä¢ {folder}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results folder found. Run experiments first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67291a1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "You've successfully:\n",
    "1. ‚úÖ Cloned repository with preprocessed data\n",
    "2. ‚úÖ Installed all dependencies\n",
    "3. ‚úÖ Verified GPU availability\n",
    "4. ‚úÖ Tested training pipeline\n",
    "5. ‚úÖ Trained recommendation models\n",
    "6. ‚úÖ Analyzed and compared results\n",
    "7. ‚úÖ Visualized learning curves\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Key Results\n",
    "\n",
    "**Dataset:** MovieLens-1M\n",
    "- 6,034 users\n",
    "- 3,533 items  \n",
    "- 1M+ ratings\n",
    "- 151,874 co-occurrence edges\n",
    "\n",
    "**Models Trained:**\n",
    "- SASRec (Transformer baseline)\n",
    "- Hybrid with Discrete Fusion (length-adaptive)\n",
    "\n",
    "**Metrics:** Hit Rate (HR), NDCG, MRR at K={5, 10, 20}\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Training Time & Epochs FAQ\n",
    "\n",
    "**Q: Why 50 epochs instead of 600 like in papers?**\n",
    "\n",
    "**A:** We use **early stopping** (patience=10):\n",
    "- Training automatically stops when validation NDCG@10 stops improving\n",
    "- With 50 epochs max ‚Üí usually converges at epoch 20-30 (~8-10 min GPU)\n",
    "- With 600 epochs max ‚Üí usually converges at epoch 30-40 (~35-45 min GPU)\n",
    "- Performance difference: ~2-3% for 10x more training time\n",
    "\n",
    "**Default (Fast):**\n",
    "```bash\n",
    "--epochs 50 --patience 10  # 8-10 min GPU, 95-98% of max performance\n",
    "```\n",
    "\n",
    "**Paper Setting (Thorough):**\n",
    "```bash\n",
    "--epochs 600 --patience 20  # 35-45 min GPU with early stopping, 100% performance\n",
    "```\n",
    "\n",
    "**Without Early Stopping (Not Recommended):**\n",
    "```bash\n",
    "--epochs 600 --patience 9999  # 80+ min GPU, risk of overfitting\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**1. Match paper settings (600 epochs):**\n",
    "```python\n",
    "!python experiments/run_experiment.py \\\n",
    "    --model hybrid_discrete \\\n",
    "    --epochs 600 \\\n",
    "    --patience 20 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.001\n",
    "```\n",
    "\n",
    "**2. Experiment with hyperparameters:**\n",
    "```python\n",
    "!python experiments/run_experiment.py \\\n",
    "    --model hybrid_discrete \\\n",
    "    --epochs 100 \\\n",
    "    --batch_size 512 \\\n",
    "    --lr 0.0005 \\\n",
    "    --d_model 128 \\\n",
    "    --n_heads 4\n",
    "```\n",
    "\n",
    "**3. Try different fusion strategies:**\n",
    "- `--model hybrid_learnable` - Per-user learned weights\n",
    "- `--model hybrid_continuous` - Neural network fusion\n",
    "- `--model hybrid_fixed` - Fixed alpha=0.5\n",
    "\n",
    "**4. Analyze specific user groups:**\n",
    "Check `results/comparison_*.csv` for performance on short/medium/long history users\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **GitHub:** https://github.com/faroukq1/length-adaptive\n",
    "- **Paper:** Length-Adaptive Hybrid Sequential Recommendation\n",
    "- **Dataset:** MovieLens-1M (GroupLens)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the README.md and EXPERIMENTS.md in the repository."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
