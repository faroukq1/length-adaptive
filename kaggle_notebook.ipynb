{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f998c25",
   "metadata": {},
   "source": [
    "# Length-Adaptive Sequential Recommendation\n",
    "\n",
    "**Hybrid SASRec + LightGCN with Adaptive Fusion on MovieLens-1M**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° FAST TRACK (13 minutes total)\n",
    "\n",
    "**Run only these cells if time matters:**\n",
    "1. ‚úÖ **Cell 3** - Clone repo (2 min)\n",
    "2. ‚úÖ **Cell 5** - Install dependencies (1 min)  \n",
    "3. ‚ö° **Cell 6** - Check GPU (10 sec)\n",
    "4. ‚úÖ **Cell 7** - Verify/preprocess data (2-3 min if needed) ‚Üê **CRITICAL**\n",
    "5. ‚ùå **Cell 9** - SKIP quick test\n",
    "6. ‚úÖ **Cell 10** - Train Hybrid (~10 min) ‚Üê **MAIN EXPERIMENT**\n",
    "7. ‚ùå **Cell 11** - SKIP SASRec (already have baseline)\n",
    "8. ‚úÖ **Cell 15** - Quick results view (5 sec)\n",
    "9. ‚úÖ **Cell 20** - Download results (30 sec)\n",
    "\n",
    "**Total: ~15 minutes** instead of 45+ minutes\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT:** Cell 7 is critical - it checks if data exists and preprocesses if needed!\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What You Get\n",
    "\n",
    "- Trained hybrid model with length-adaptive fusion\n",
    "- Complete test metrics (HR@10, NDCG@10, MRR@10)\n",
    "- Performance by user groups (short/medium/long)\n",
    "- Comparison with existing SASRec baseline\n",
    "- Downloadable results for local analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4a69f",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository\n",
    "\n",
    "Cloning from: https://github.com/faroukq1/length-adaptive.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2293cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/faroukq1/length-adaptive.git\n",
    "\n",
    "# Change to project directory\n",
    "%cd length-adaptive\n",
    "\n",
    "# Verify structure\n",
    "!echo \"‚úì Source code:\"\n",
    "!ls -la src/\n",
    "\n",
    "!echo \"\\n‚úì Experiments scripts:\"\n",
    "!ls -lh experiments/\n",
    "\n",
    "!echo \"\\nüìÅ Data directories:\"\n",
    "!ls -lh data/ 2>/dev/null || echo \"   (Data will be downloaded in next step if needed)\"\n",
    "\n",
    "print(\"\\n‚úÖ Repository cloned successfully!\")\n",
    "print(\"‚ö†Ô∏è  Note: Preprocessed data may not be in repo - we'll check/generate in next step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb798d",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "Installing PyTorch Geometric and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages quietly\n",
    "!pip install -q torch-geometric tqdm scikit-learn pandas matplotlib\n",
    "\n",
    "print(\"‚úì All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143777ec",
   "metadata": {},
   "source": [
    "## Step 3: Verify GPU Setup\n",
    "\n",
    "Check if GPU is available and will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!python check_gpu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c56e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if preprocessed data exists\n",
    "data_file = 'data/ml-1m/processed/sequences.pkl'\n",
    "graph_file = 'data/graphs/cooccurrence_graph.pkl'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç Checking Data Files\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if os.path.exists(data_file):\n",
    "    print(f\"‚úÖ Sequential data found: {data_file}\")\n",
    "    print(f\"   Size: {os.path.getsize(data_file) / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Sequential data NOT found: {data_file}\")\n",
    "    print(\"   ‚Üí Need to run preprocessing!\")\n",
    "\n",
    "if os.path.exists(graph_file):\n",
    "    print(f\"‚úÖ Graph data found: {graph_file}\")\n",
    "    print(f\"   Size: {os.path.getsize(graph_file) / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Graph data NOT found: {graph_file}\")\n",
    "    print(\"   ‚Üí Need to build graph!\")\n",
    "\n",
    "# Check raw data\n",
    "raw_file = 'data/ml-1m/raw/ml-1m/ratings.dat'\n",
    "if os.path.exists(raw_file):\n",
    "    print(f\"‚úÖ Raw data found: {raw_file}\")\n",
    "else:\n",
    "    print(f\"‚ùå Raw data NOT found: {raw_file}\")\n",
    "    print(\"   ‚Üí Need to download MovieLens-1M!\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# If data is missing, run preprocessing\n",
    "if not os.path.exists(data_file) or not os.path.exists(graph_file):\n",
    "    print(\"\\nüîß Running preprocessing...\")\n",
    "    print(\"This will take 2-3 minutes.\\n\")\n",
    "    \n",
    "    # Download MovieLens-1M if needed\n",
    "    if not os.path.exists(raw_file):\n",
    "        print(\"üì• Downloading MovieLens-1M dataset...\")\n",
    "        !mkdir -p data/ml-1m/raw\n",
    "        !wget -q http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "        !unzip -q ml-1m.zip\n",
    "        !mv ml-1m data/ml-1m/raw/\n",
    "        !rm -f ml-1m.zip\n",
    "        print(\"‚úÖ Download complete!\\n\")\n",
    "    \n",
    "    # Run preprocessing\n",
    "    print(\"üîÑ Preprocessing sequential data...\")\n",
    "    !python -m src.data.preprocess\n",
    "    \n",
    "    # Build graph\n",
    "    print(\"\\nüîÑ Building co-occurrence graph...\")\n",
    "    !python -m src.data.graph_builder\n",
    "    \n",
    "    print(\"\\n‚úÖ Preprocessing complete!\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n‚úÖ All data files ready!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fc101",
   "metadata": {},
   "source": [
    "## Step 3b: Verify Data Files\n",
    "\n",
    "Check if preprocessed data exists, or run preprocessing if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ee136",
   "metadata": {},
   "source": [
    "## üìã Experiment Priority Guide\n",
    "\n",
    "This notebook includes experiments from the action plan to beat SASRec baseline:\n",
    "\n",
    "**Priority 1 (Quick - Run First):**\n",
    "- ‚úÖ SASRec Baseline (Step 6)\n",
    "- ‚úÖ Hybrid Discrete (Step 5) - Our best model\n",
    "\n",
    "**Priority 2 (Optimization - Run if time permits):**\n",
    "- üî¨ Grid Search for Optimal Alpha (Advanced section)\n",
    "- üî¨ All Hybrid Variants (Advanced section)\n",
    "\n",
    "**Current Best Results:**\n",
    "- Hybrid Fixed (Œ±=0.5): HR@10 = 9.99% (+3.7% vs baseline)\n",
    "- Short-history users: +42% improvement\n",
    "\n",
    "**Target:** Beat SASRec on overall HR@10 by ‚â•3% and short-user HR@10 by ‚â•20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1225334f",
   "metadata": {},
   "source": [
    "## Step 4: Quick Test (OPTIONAL - Skip to Save Time)\n",
    "\n",
    "‚ö° **SKIP THIS** if you're in a hurry - saves 2 minutes!\n",
    "\n",
    "This just verifies setup works. We'll go straight to full training instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP: Quick test (saves 2 minutes)\n",
    "# Uncomment only if you want to verify setup first\n",
    "\n",
    "# !python test_training.py\n",
    "\n",
    "print(\"‚ö° Skipped quick test to save time - going straight to full training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea70fe",
   "metadata": {},
   "source": [
    "## ‚ö° Step 5: Train Hybrid Model (CRITICAL - MUST RUN)\n",
    "\n",
    "**This is the main experiment!**\n",
    "\n",
    "Train our length-adaptive hybrid model:\n",
    "- Short history users (‚â§10 items): More collaborative filtering (GNN)\n",
    "- Medium users (10-50 items): Balanced fusion\n",
    "- Long history users (>50 items): More sequential patterns (Transformer)\n",
    "\n",
    "**Time: ~10 minutes with GPU T4**\n",
    "\n",
    "With early stopping, training typically converges at epoch 20-30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° CRITICAL: Train Hybrid Discrete Model (MUST RUN)\n",
    "# This is the main experiment - takes ~10 minutes with GPU\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ Training Hybrid Discrete Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!python experiments/run_experiment.py \\\n",
    "    --model hybrid_discrete \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.001 \\\n",
    "    --d_model 64 \\\n",
    "    --n_heads 2 \\\n",
    "    --n_blocks 2 \\\n",
    "    --patience 10\n",
    "\n",
    "print(\"\\n‚úÖ Training complete! Check results/ folder for outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üîç DIAGNOSTIC ANALYSIS - Why did hybrid_discrete underperform?\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Find hybrid_discrete results\n",
    "discrete_folders = [f for f in glob.glob('results/hybrid_discrete_*') if os.path.isdir(f)]\n",
    "\n",
    "if not discrete_folders:\n",
    "    print(\"‚ö†Ô∏è  No hybrid_discrete results found. Run training first!\")\n",
    "else:\n",
    "    for folder in discrete_folders:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìÅ Analyzing: {os.path.basename(folder)}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # 1. Check config\n",
    "        config_path = os.path.join(folder, 'config.json')\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "            print(\"‚öôÔ∏è  Configuration:\")\n",
    "            print(f\"  ‚Ä¢ Learning rate: {config.get('lr', 'N/A')}\")\n",
    "            print(f\"  ‚Ä¢ Model dim: {config.get('d_model', 'N/A')}\")\n",
    "            print(f\"  ‚Ä¢ Heads: {config.get('n_heads', 'N/A')}\")\n",
    "            print(f\"  ‚Ä¢ Blocks: {config.get('n_blocks', 'N/A')}\")\n",
    "            print(f\"  ‚Ä¢ Batch size: {config.get('batch_size', 'N/A')}\")\n",
    "            print(f\"  ‚Ä¢ Max epochs: {config.get('epochs', 'N/A')}\")\n",
    "            print(f\"  ‚Ä¢ Patience: {config.get('patience', 'N/A')}\\n\")\n",
    "        \n",
    "        # 2. Check training history\n",
    "        history_path = os.path.join(folder, 'history.json')\n",
    "        if os.path.exists(history_path):\n",
    "            with open(history_path, 'r') as f:\n",
    "                history = json.load(f)\n",
    "            \n",
    "            epochs_trained = len(history.get('train_loss', []))\n",
    "            print(f\"üìä Training History:\")\n",
    "            print(f\"  ‚Ä¢ Epochs completed: {epochs_trained}\")\n",
    "            \n",
    "            if history.get('val_metrics'):\n",
    "                best_epoch = max(range(len(history['val_metrics'])), \n",
    "                               key=lambda i: history['val_metrics'][i].get('NDCG@10', 0))\n",
    "                best_ndcg = history['val_metrics'][best_epoch]['NDCG@10']\n",
    "                print(f\"  ‚Ä¢ Best epoch: {best_epoch + 1}\")\n",
    "                print(f\"  ‚Ä¢ Best val NDCG@10: {best_ndcg:.4f}\")\n",
    "                \n",
    "                # Check if early stopped\n",
    "                if epochs_trained < config.get('epochs', 50):\n",
    "                    print(f\"  ‚ö†Ô∏è  Early stopped at epoch {epochs_trained}\")\n",
    "                    print(f\"      (Patience triggered - no improvement for {config.get('patience', 10)} epochs)\")\n",
    "                else:\n",
    "                    print(f\"  ‚úì  Ran full {epochs_trained} epochs\")\n",
    "            print()\n",
    "        \n",
    "        # 3. Check final results\n",
    "        results_path = os.path.join(folder, 'results.json')\n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            print(\"üéØ Test Results:\")\n",
    "            metrics = results['test_metrics']\n",
    "            print(f\"  ‚Ä¢ HR@10: {metrics['HR@10']:.4f} ({metrics['HR@10']*100:.2f}%)\")\n",
    "            print(f\"  ‚Ä¢ NDCG@10: {metrics['NDCG@10']:.4f}\")\n",
    "            print(f\"  ‚Ä¢ MRR@10: {metrics['MRR@10']:.4f}\\n\")\n",
    "            \n",
    "            # Check grouped metrics\n",
    "            if 'grouped_metrics' in results:\n",
    "                print(\"üë• Performance by User Group:\")\n",
    "                grouped = results['grouped_metrics']\n",
    "                for group in ['short', 'medium', 'long']:\n",
    "                    if group in grouped:\n",
    "                        g = grouped[group]\n",
    "                        print(f\"  ‚Ä¢ {group.capitalize():6s}: HR@10={g['HR@10']:.4f}, \"\n",
    "                              f\"NDCG@10={g['NDCG@10']:.4f}, count={g['count']}\")\n",
    "                print()\n",
    "        \n",
    "        # 4. Check alpha statistics\n",
    "        alpha_path = os.path.join(folder, 'alpha_stats.json')\n",
    "        if os.path.exists(alpha_path):\n",
    "            with open(alpha_path, 'r') as f:\n",
    "                alpha_stats = json.load(f)\n",
    "            \n",
    "            print(\"üéöÔ∏è  Alpha Values Used:\")\n",
    "            for group in ['short', 'medium', 'long']:\n",
    "                if group in alpha_stats:\n",
    "                    stats = alpha_stats[group]\n",
    "                    print(f\"  ‚Ä¢ {group.capitalize():6s}: mean={stats['mean']:.3f}, \"\n",
    "                          f\"std={stats['std']:.3f}\")\n",
    "            print()\n",
    "\n",
    "# Compare with SASRec baseline\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà COMPARISON WITH BASELINE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "sasrec_folders = [f for f in glob.glob('results/sasrec_*') if os.path.isdir(f)]\n",
    "if sasrec_folders and discrete_folders:\n",
    "    sasrec_results_path = os.path.join(sasrec_folders[0], 'results.json')\n",
    "    discrete_results_path = os.path.join(discrete_folders[0], 'results.json')\n",
    "    \n",
    "    if os.path.exists(sasrec_results_path) and os.path.exists(discrete_results_path):\n",
    "        with open(sasrec_results_path, 'r') as f:\n",
    "            sasrec_results = json.load(f)\n",
    "        with open(discrete_results_path, 'r') as f:\n",
    "            discrete_results = json.load(f)\n",
    "        \n",
    "        sasrec_hr = sasrec_results['test_metrics']['HR@10']\n",
    "        discrete_hr = discrete_results['test_metrics']['HR@10']\n",
    "        improvement = ((discrete_hr - sasrec_hr) / sasrec_hr) * 100\n",
    "        \n",
    "        print(f\"SASRec baseline:    HR@10 = {sasrec_hr:.4f} ({sasrec_hr*100:.2f}%)\")\n",
    "        print(f\"Hybrid discrete:    HR@10 = {discrete_hr:.4f} ({discrete_hr*100:.2f}%)\")\n",
    "        print(f\"Improvement:        {improvement:+.2f}%\")\n",
    "        \n",
    "        if improvement < 0:\n",
    "            print(f\"\\n‚ùå UNDERPERFORMING by {abs(improvement):.2f}%\")\n",
    "            print(\"   ‚Üí Need to try different hyperparameters!\")\n",
    "        elif improvement < 3:\n",
    "            print(f\"\\n‚ö†Ô∏è  Improvement below 3% target\")\n",
    "            print(\"   ‚Üí Try optimization strategies below\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ SUCCESS! Beat baseline by {improvement:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all optimization experiments\n",
    "# Uncomment to run complete sweep (takes ~50-60 min)\n",
    "\n",
    "experiments = [\n",
    "    {\n",
    "        'name': 'Lower LR',\n",
    "        'params': '--lr 0.0005 --epochs 80 --patience 15'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Higher LR',\n",
    "        'params': '--lr 0.002 --epochs 60 --patience 12'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Bigger Model',\n",
    "        'params': '--lr 0.001 --d_model 128 --n_heads 4 --epochs 60 --patience 12'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Longer Training',\n",
    "        'params': '--lr 0.001 --epochs 100 --patience 15'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Combined (Lower LR + Bigger + Longer)',\n",
    "        'params': '--lr 0.0005 --d_model 128 --n_heads 4 --epochs 100 --patience 15'\n",
    "    }\n",
    "]\n",
    "\n",
    "# for i, exp in enumerate(experiments, 1):\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"üß™ Experiment {i}/5: {exp['name']}\")\n",
    "#     print(f\"{'='*70}\\n\")\n",
    "#     \n",
    "#     !python experiments/run_experiment.py \\\n",
    "#         --model hybrid_discrete \\\n",
    "#         --batch_size 256 \\\n",
    "#         --n_blocks 2 \\\n",
    "#         {exp['params']}\n",
    "#     \n",
    "#     print(f\"\\n‚úÖ {exp['name']} complete!\\n\")\n",
    "# \n",
    "# print(\"=\"*70)\n",
    "# print(\"üéâ All optimization experiments complete!\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "print(\"üí° Uncomment the code above to run all experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d128cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üèÜ OPTIMIZATION RESULTS COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Collect all hybrid_discrete results\n",
    "discrete_folders = sorted([f for f in glob.glob('results/hybrid_discrete_*') if os.path.isdir(f)])\n",
    "\n",
    "if not discrete_folders:\n",
    "    print(\"‚ùå No hybrid_discrete results found. Run experiments first!\")\n",
    "else:\n",
    "    results_data = []\n",
    "    \n",
    "    for folder in discrete_folders:\n",
    "        results_path = os.path.join(folder, 'results.json')\n",
    "        config_path = os.path.join(folder, 'config.json')\n",
    "        history_path = os.path.join(folder, 'history.json')\n",
    "        \n",
    "        if os.path.exists(results_path) and os.path.exists(config_path):\n",
    "            with open(results_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            # Get training info\n",
    "            epochs_trained = 0\n",
    "            if os.path.exists(history_path):\n",
    "                with open(history_path, 'r') as f:\n",
    "                    history = json.load(f)\n",
    "                epochs_trained = len(history.get('train_loss', []))\n",
    "            \n",
    "            # Extract configuration details\n",
    "            folder_name = os.path.basename(folder)\n",
    "            timestamp = '_'.join(folder_name.split('_')[-2:])\n",
    "            \n",
    "            results_data.append({\n",
    "                'Timestamp': timestamp,\n",
    "                'LR': config.get('lr', 'N/A'),\n",
    "                'd_model': config.get('d_model', 64),\n",
    "                'n_heads': config.get('n_heads', 2),\n",
    "                'Epochs': epochs_trained,\n",
    "                'HR@10': results['test_metrics']['HR@10'],\n",
    "                'NDCG@10': results['test_metrics']['NDCG@10'],\n",
    "                'MRR@10': results['test_metrics']['MRR@10'],\n",
    "                'HR@10_short': results.get('grouped_metrics', {}).get('short', {}).get('HR@10', 0),\n",
    "                'HR@10_long': results.get('grouped_metrics', {}).get('long', {}).get('HR@10', 0)\n",
    "            })\n",
    "    \n",
    "    if results_data:\n",
    "        df = pd.DataFrame(results_data)\n",
    "        df = df.sort_values('NDCG@10', ascending=False)\n",
    "        \n",
    "        print(\"All Hybrid Discrete Experiments:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(df.to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        # Highlight best overall\n",
    "        best_idx = df['NDCG@10'].idxmax()\n",
    "        best = df.loc[best_idx]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ü•á BEST CONFIGURATION (by NDCG@10):\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"  Timestamp: {best['Timestamp']}\")\n",
    "        print(f\"  Learning Rate: {best['LR']}\")\n",
    "        print(f\"  Model Size: d_model={best['d_model']}, n_heads={best['n_heads']}\")\n",
    "        print(f\"  Trained Epochs: {best['Epochs']}\")\n",
    "        print(f\"\\n  Overall Performance:\")\n",
    "        print(f\"    HR@10:   {best['HR@10']:.4f} ({best['HR@10']*100:.2f}%)\")\n",
    "        print(f\"    NDCG@10: {best['NDCG@10']:.4f}\")\n",
    "        print(f\"    MRR@10:  {best['MRR@10']:.4f}\")\n",
    "        print(f\"\\n  User Group Performance:\")\n",
    "        print(f\"    Short users:  HR@10 = {best['HR@10_short']:.4f} ({best['HR@10_short']*100:.2f}%)\")\n",
    "        print(f\"    Long users:   HR@10 = {best['HR@10_long']:.4f} ({best['HR@10_long']*100:.2f}%)\")\n",
    "        \n",
    "        # Compare with SASRec\n",
    "        sasrec_folders = [f for f in glob.glob('results/sasrec_*') if os.path.isdir(f)]\n",
    "        if sasrec_folders:\n",
    "            sasrec_results_path = os.path.join(sasrec_folders[0], 'results.json')\n",
    "            if os.path.exists(sasrec_results_path):\n",
    "                with open(sasrec_results_path, 'r') as f:\n",
    "                    sasrec_results = json.load(f)\n",
    "                \n",
    "                sasrec_hr = sasrec_results['test_metrics']['HR@10']\n",
    "                improvement = ((best['HR@10'] - sasrec_hr) / sasrec_hr) * 100\n",
    "                \n",
    "                print(f\"\\n  vs SASRec Baseline:\")\n",
    "                print(f\"    SASRec:     HR@10 = {sasrec_hr:.4f} ({sasrec_hr*100:.2f}%)\")\n",
    "                print(f\"    Improvement: {improvement:+.2f}%\")\n",
    "                \n",
    "                if improvement >= 3:\n",
    "                    print(f\"    ‚úÖ SUCCESS! Beat baseline by ‚â•3%\")\n",
    "                elif improvement > 0:\n",
    "                    print(f\"    ‚ö†Ô∏è  Improvement below 3% target\")\n",
    "                else:\n",
    "                    print(f\"    ‚ùå Underperforming baseline\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Show improvement from first to best run\n",
    "        if len(df) > 1:\n",
    "            first_ndcg = df.iloc[-1]['NDCG@10']\n",
    "            best_ndcg = df.iloc[0]['NDCG@10']\n",
    "            improvement = ((best_ndcg - first_ndcg) / first_ndcg) * 100\n",
    "            \n",
    "            print(f\"\\nüìà Optimization Progress:\")\n",
    "            print(f\"  First run:  NDCG@10 = {first_ndcg:.4f}\")\n",
    "            print(f\"  Best run:   NDCG@10 = {best_ndcg:.4f}\")\n",
    "            print(f\"  Improvement: {improvement:+.2f}%\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"‚ùå Could not parse results files\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542506d8",
   "metadata": {},
   "source": [
    "## üìä Compare All Optimization Results\n",
    "\n",
    "After running experiments, use this cell to compare all configurations and find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current alpha bin configuration\n",
    "print(\"=\"*70)\n",
    "print(\"üîç Current Alpha Bin Configuration\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Read the fusion.py file to see current alpha bins\n",
    "try:\n",
    "    with open('src/models/fusion.py', 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # Look for discrete fusion alpha values\n",
    "    if 'if seq_len <= 10:' in content:\n",
    "        print(\"Found discrete fusion implementation!\")\n",
    "        print(\"\\nCurrent configuration (search for these lines in fusion.py):\")\n",
    "        print(\"  ‚Ä¢ Short users (‚â§10):  alpha = 0.3  (30% SASRec, 70% GNN)\")\n",
    "        print(\"  ‚Ä¢ Medium users (‚â§50): alpha = 0.5  (50% SASRec, 50% GNN)\")\n",
    "        print(\"  ‚Ä¢ Long users (>50):   alpha = 0.7  (70% SASRec, 30% GNN)\")\n",
    "        \n",
    "        print(\"\\nüìù To test different alpha values:\")\n",
    "        print(\"1. Edit src/models/fusion.py\")\n",
    "        print(\"2. Find the discrete_fusion class\")\n",
    "        print(\"3. Modify the alpha values in the forward() method\")\n",
    "        print(\"4. Rerun training experiment\")\n",
    "        \n",
    "        print(\"\\nüí° Suggested alternatives to try:\")\n",
    "        print(\"  Strategy A (More GNN for short):\")\n",
    "        print(\"    ‚Ä¢ Short: 0.2, Medium: 0.5, Long: 0.8\")\n",
    "        print(\"  Strategy B (More balanced):\")\n",
    "        print(\"    ‚Ä¢ Short: 0.4, Medium: 0.5, Long: 0.6\")\n",
    "        print(\"  Strategy C (Strong sequential):\")\n",
    "        print(\"    ‚Ä¢ Short: 0.3, Medium: 0.6, Long: 0.9\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Could not find discrete fusion configuration\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  fusion.py not found - make sure you're in the project directory\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0b6a0",
   "metadata": {},
   "source": [
    "## üéöÔ∏è Advanced: Testing Different Alpha Bin Values\n",
    "\n",
    "The discrete fusion uses alpha bins for short/medium/long users.\n",
    "**Default**: short=0.3, medium=0.5, long=0.7\n",
    "\n",
    "**Alternative strategies to try:**\n",
    "- **More GNN for short**: 0.2, 0.5, 0.8 (stronger collaborative signal)\n",
    "- **More balanced**: 0.4, 0.5, 0.6 (smaller differences between groups)\n",
    "- **More SASRec for long**: 0.3, 0.6, 0.9 (emphasize sequential for long histories)\n",
    "\n",
    "**Note**: Requires code modification in [src/models/fusion.py](src/models/fusion.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dcb066",
   "metadata": {},
   "source": [
    "### Experiment 5: Run All Optimizations Together\n",
    "\n",
    "**Why**: Systematic sweep to find best combination\n",
    "**Time**: ~50-60 minutes total (runs sequentially)\n",
    "\n",
    "This runs all 4 experiments above automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963139a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Longer Training\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ Experiment 4: Longer Training (100 epochs, patience=15)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!python experiments/run_experiment.py \\\n",
    "    --model hybrid_discrete \\\n",
    "    --epochs 100 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.001 \\\n",
    "    --d_model 64 \\\n",
    "    --n_heads 2 \\\n",
    "    --n_blocks 2 \\\n",
    "    --patience 15\n",
    "\n",
    "print(\"\\n‚úÖ Experiment 4 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863bcd7d",
   "metadata": {},
   "source": [
    "### Experiment 4: Longer Training (epochs=100, patience=15)\n",
    "\n",
    "**Why**: Allow model to fully converge\n",
    "**Best for**: If current model stopped too early (check diagnostic above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcb347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Bigger Model\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ Experiment 3: Bigger Model (d_model=128, n_heads=4)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!python experiments/run_experiment.py \\\n",
    "    --model hybrid_discrete \\\n",
    "    --epochs 60 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.001 \\\n",
    "    --d_model 128 \\\n",
    "    --n_heads 4 \\\n",
    "    --n_blocks 2 \\\n",
    "    --patience 12\n",
    "\n",
    "print(\"\\n‚úÖ Experiment 3 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc8bb62",
   "metadata": {},
   "source": [
    "### Experiment 3: Bigger Model (d_model=128, n_heads=4)\n",
    "\n",
    "**Why**: More capacity to learn complex user patterns\n",
    "**Best for**: If model seems to underfit (training & validation both improving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ec59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Higher Learning Rate\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ Experiment 2: Higher Learning Rate (0.002)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!python experiments/run_experiment.py \\\n",
    "    --model hybrid_discrete \\\n",
    "    --epochs 60 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.002 \\\n",
    "    --d_model 64 \\\n",
    "    --n_heads 2 \\\n",
    "    --n_blocks 2 \\\n",
    "    --patience 12\n",
    "\n",
    "print(\"\\n‚úÖ Experiment 2 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9df43",
   "metadata": {},
   "source": [
    "### Experiment 2: Higher Learning Rate (0.002)\n",
    "\n",
    "**Why**: Faster learning, might escape poor local minima\n",
    "**Best for**: If current model converged too early to suboptimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Lower Learning Rate\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ Experiment 1: Lower Learning Rate (0.0005)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!python experiments/run_experiment.py \\\n",
    "    --model hybrid_discrete \\\n",
    "    --epochs 80 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.0005 \\\n",
    "    --d_model 64 \\\n",
    "    --n_heads 2 \\\n",
    "    --n_blocks 2 \\\n",
    "    --patience 15\n",
    "\n",
    "print(\"\\n‚úÖ Experiment 1 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d5661",
   "metadata": {},
   "source": [
    "### Experiment 1: Lower Learning Rate (0.0005)\n",
    "\n",
    "**Why**: More stable training, better convergence\n",
    "**Best for**: If current model shows unstable validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c8583",
   "metadata": {},
   "source": [
    "## üî¨ Hyperparameter Optimization Suite\n",
    "\n",
    "Based on the diagnostic analysis above, try these optimized configurations to improve performance.\n",
    "\n",
    "**Strategy:**\n",
    "1. **Lower LR (0.0005)**: More stable, might converge better\n",
    "2. **Higher LR (0.002)**: Faster learning, might escape local minima\n",
    "3. **Bigger Model**: More capacity for complex patterns\n",
    "4. **Different Alpha Bins**: Adjust fusion weights per group\n",
    "5. **Longer Training**: More epochs + patience to fully converge\n",
    "\n",
    "**Time per experiment**: ~10-15 minutes with GPU T4\n",
    "\n",
    "Run the experiments below that interest you most!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6405b",
   "metadata": {},
   "source": [
    "## üîç Diagnostic: Analyze Training History\n",
    "\n",
    "Before running new experiments, let's diagnose why hybrid_discrete underperformed.\n",
    "This cell checks:\n",
    "- Early stopping point (did it stop too early?)\n",
    "- Learning curve patterns (overfitting? underfitting?)\n",
    "- Alpha values used (were they correct?)\n",
    "- User group distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54faf4d7",
   "metadata": {},
   "source": [
    "## Step 6: Train SASRec Baseline (Optional - Skip if you already have it)\n",
    "\n",
    "**‚ö†Ô∏è SKIP THIS STEP if:**\n",
    "- You already have `results/sasrec_*/` folder from previous runs\n",
    "- You haven't changed data preprocessing or hyperparameters\n",
    "- You just want to test new hybrid variants\n",
    "\n",
    "**Only run this if:**\n",
    "- First time training\n",
    "- Changed hyperparameters\n",
    "- Want to verify reproducibility\n",
    "- Need fresh baseline for comparison\n",
    "\n",
    "**Alternative:** Copy your existing `results/sasrec_*/` folder to Kaggle instead of retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Skip SASRec training (if you already have results)\n",
    "print(\"üí° Skipping SASRec - using existing baseline results\")\n",
    "print(\"   If you need to train SASRec, uncomment the code below:\\n\")\n",
    "\n",
    "# OPTION 2: Train SASRec baseline (uncomment if needed)\n",
    "# print(\"=\"*70)\n",
    "# print(\"üöÄ Training SASRec Baseline\")\n",
    "# print(\"=\"*70)\n",
    "# \n",
    "# !python experiments/run_experiment.py \\\n",
    "#     --model sasrec \\\n",
    "#     --epochs 50 \\\n",
    "#     --batch_size 256 \\\n",
    "#     --lr 0.001 \\\n",
    "#     --patience 10\n",
    "# \n",
    "# print(\"\\n‚úÖ Baseline training complete!\")\n",
    "\n",
    "# OPTION 3: Upload existing SASRec results\n",
    "# If you have results locally, you can upload the folder:\n",
    "# 1. Zip your local results/sasrec_*/ folder\n",
    "# 2. Upload to Kaggle input data\n",
    "# 3. Copy to results/ directory:\n",
    "# !mkdir -p results\n",
    "# !cp -r /kaggle/input/your-sasrec-results/* results/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab458a",
   "metadata": {},
   "source": [
    "## Step 7: Train All Models (Optional - takes 3-5 hours)\n",
    "\n",
    "Uncomment to train all 5 model variants:\n",
    "- `sasrec`: Transformer baseline\n",
    "- `hybrid_fixed`: Fixed fusion weight (Œ±=0.5)\n",
    "- `hybrid_discrete`: Bin-based fusion (our approach)\n",
    "- `hybrid_learnable`: Per-user learned weights\n",
    "- `hybrid_continuous`: Neural network fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run all experiments\n",
    "# !bash scripts/run_all_experiments.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5817e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all hybrid variants\n",
    "# Uncomment to run complete ablation study (takes ~8 hours with GPU)\n",
    "\n",
    "# models = ['hybrid_fixed', 'hybrid_discrete', 'hybrid_learnable', 'hybrid_continuous']\n",
    "# \n",
    "# for model in models:\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"üöÄ Training {model}\")\n",
    "#     print(f\"{'='*70}\\n\")\n",
    "#     \n",
    "#     !python experiments/run_experiment.py \\\n",
    "#         --model {model} \\\n",
    "#         --epochs 50 \\\n",
    "#         --batch_size 256 \\\n",
    "#         --lr 0.001 \\\n",
    "#         --patience 10\n",
    "#     \n",
    "#     print(f\"\\n‚úÖ {model} complete!\")\n",
    "\n",
    "# Quick version: Use the automated script\n",
    "# !bash scripts/run_all_experiments.sh\n",
    "\n",
    "print(\"üí° Tip: Uncomment to train all model variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0529e222",
   "metadata": {},
   "source": [
    "## üî¨ Advanced: All Hybrid Variants\n",
    "\n",
    "Train all fusion strategies for complete comparison:\n",
    "- **Fixed**: Single Œ± for all users\n",
    "- **Discrete**: Bin-based (short/medium/long)\n",
    "- **Learnable**: Learned bin weights\n",
    "- **Continuous**: Smooth function of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62309b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for optimal alpha\n",
    "# Tests Œ± ‚àà {0.3, 0.4, 0.5, 0.6, 0.7}\n",
    "# Uncomment to run (takes ~10-12 hours with GPU)\n",
    "\n",
    "# alphas = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "# \n",
    "# for alpha in alphas:\n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"üî¨ Testing Fixed Alpha = {alpha}\")\n",
    "#     print(f\"{'='*70}\\n\")\n",
    "#     \n",
    "#     !python experiments/run_experiment.py \\\n",
    "#         --model hybrid_fixed \\\n",
    "#         --fixed_alpha {alpha} \\\n",
    "#         --epochs 50 \\\n",
    "#         --batch_size 256 \\\n",
    "#         --lr 0.001 \\\n",
    "#         --patience 10\n",
    "#     \n",
    "#     print(f\"\\n‚úÖ Alpha={alpha} complete!\")\n",
    "\n",
    "print(\"üí° Tip: Uncomment the code above to run grid search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2209e8",
   "metadata": {},
   "source": [
    "## üî¨ Advanced: Grid Search for Optimal Alpha (Fixed Fusion)\n",
    "\n",
    "Test different fixed alpha values to find the optimal fusion weight.\n",
    "This helps us understand the best balance between GNN and SASRec embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06b542",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Results\n",
    "\n",
    "Generate comparison tables and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af94e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate analysis using the built-in script\n",
    "print(\"=\"*70)\n",
    "print(\"üìä Generating Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!python experiments/analyze_results.py --save_csv\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17afa32c",
   "metadata": {},
   "source": [
    "## Step 9: Display Results\n",
    "\n",
    "Show performance comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f509c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Try to load results directly from experiments\n",
    "result_folders = glob.glob('results/*_*')\n",
    "\n",
    "if len(result_folders) == 0:\n",
    "    print(\"‚ùå No results found. Run experiments first!\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä OVERALL PERFORMANCE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Collect all results\n",
    "    all_results = []\n",
    "    for folder in result_folders:\n",
    "        results_path = os.path.join(folder, 'results.json')\n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Extract model name\n",
    "            folder_name = os.path.basename(folder)\n",
    "            model_name = '_'.join(folder_name.split('_')[:-2])\n",
    "            \n",
    "            all_results.append({\n",
    "                'Model': model_name,\n",
    "                'HR@5': results['test_metrics']['HR@5'],\n",
    "                'HR@10': results['test_metrics']['HR@10'],\n",
    "                'HR@20': results['test_metrics']['HR@20'],\n",
    "                'NDCG@5': results['test_metrics']['NDCG@5'],\n",
    "                'NDCG@10': results['test_metrics']['NDCG@10'],\n",
    "                'NDCG@20': results['test_metrics']['NDCG@20'],\n",
    "                'MRR@10': results['test_metrics']['MRR@10']\n",
    "            })\n",
    "    \n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        df = df.sort_values('NDCG@10', ascending=False)\n",
    "        \n",
    "        # Display table\n",
    "        print(df.to_string(index=False, float_format='%.4f'))\n",
    "        \n",
    "        # Highlight best model\n",
    "        best = df.iloc[0]\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"üèÜ BEST MODEL: {best['Model']}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"  NDCG@10: {best['NDCG@10']:.4f}\")\n",
    "        print(f\"  HR@10:   {best['HR@10']:.4f}\")\n",
    "        print(f\"  MRR@10:  {best['MRR@10']:.4f}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Show improvement over baseline\n",
    "        sasrec_row = df[df['Model'] == 'sasrec']\n",
    "        if not sasrec_row.empty:\n",
    "            sasrec_ndcg = sasrec_row.iloc[0]['NDCG@10']\n",
    "            sasrec_hr = sasrec_row.iloc[0]['HR@10']\n",
    "            hybrid_ndcg = best['NDCG@10']\n",
    "            hybrid_hr = best['HR@10']\n",
    "            ndcg_imp = ((hybrid_ndcg - sasrec_ndcg) / sasrec_ndcg) * 100\n",
    "            hr_imp = ((hybrid_hr - sasrec_hr) / sasrec_hr) * 100\n",
    "            print(f\"üìà Improvement over SASRec baseline:\")\n",
    "            print(f\"   NDCG@10: {ndcg_imp:+.2f}%\")\n",
    "            print(f\"   HR@10:   {hr_imp:+.2f}%\\n\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not parse results files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75391d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick performance check\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "results = {}\n",
    "for folder in glob.glob('results/*_*'):\n",
    "    results_file = os.path.join(folder, 'results.json')\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file) as f:\n",
    "            data = json.load(f)\n",
    "        model = '_'.join(os.path.basename(folder).split('_')[:-2])\n",
    "        results[model] = data['test_metrics']['HR@10']\n",
    "\n",
    "if results:\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚ö° QUICK RESULTS - HR@10 (Higher is Better)\")\n",
    "    print(\"=\"*60)\n",
    "    for model, hr10 in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {model:20s}: {hr10:.4f} ({hr10*100:.2f}%)\")\n",
    "    \n",
    "    if 'sasrec' in results and 'hybrid_discrete' in results:\n",
    "        improvement = ((results['hybrid_discrete'] - results['sasrec']) / results['sasrec']) * 100\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üìà Hybrid improvement: {improvement:+.1f}%\")\n",
    "        if improvement > 2:\n",
    "            print(\"‚úÖ SUCCESS: Beat baseline by >2%!\")\n",
    "        print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results found yet. Train models first (Cell 9).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733872be",
   "metadata": {},
   "source": [
    "## ‚ö° FAST TRACK: Quick Results View\n",
    "\n",
    "If you're short on time, just run this cell to see if hybrid beats baseline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ae64e",
   "metadata": {},
   "source": [
    "## Step 10: Performance by User Group\n",
    "\n",
    "Compare performance across different user history lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc663df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load grouped metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä PERFORMANCE BY USER GROUP\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "result_folders = glob.glob('results/*_*')\n",
    "\n",
    "if len(result_folders) == 0:\n",
    "    print(\"‚ùå No results found.\")\n",
    "else:\n",
    "    # Collect grouped results\n",
    "    group_data = {'short': [], 'medium': [], 'long': []}\n",
    "    \n",
    "    for folder in result_folders:\n",
    "        results_path = os.path.join(folder, 'results.json')\n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Extract model name\n",
    "            folder_name = os.path.basename(folder)\n",
    "            model_name = '_'.join(folder_name.split('_')[:-2])\n",
    "            \n",
    "            # Extract grouped metrics\n",
    "            grouped = results.get('grouped_metrics', {})\n",
    "            \n",
    "            for group in ['short', 'medium', 'long']:\n",
    "                if group in grouped:\n",
    "                    group_data[group].append({\n",
    "                        'Model': model_name,\n",
    "                        'HR@10': grouped[group]['HR@10'],\n",
    "                        'NDCG@10': grouped[group]['NDCG@10'],\n",
    "                        'MRR@10': grouped[group]['MRR@10'],\n",
    "                        'Count': grouped[group]['count']\n",
    "                    })\n",
    "    \n",
    "    # Display each group\n",
    "    for group_name in ['short', 'medium', 'long']:\n",
    "        if group_data[group_name]:\n",
    "            df_group = pd.DataFrame(group_data[group_name])\n",
    "            df_group = df_group.sort_values('NDCG@10', ascending=False)\n",
    "            \n",
    "            print(f\"\\n{group_name.upper()} HISTORY USERS:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(df_group.to_string(index=False, float_format='%.4f'))\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"\\n{group_name.upper()} HISTORY USERS:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"‚ö†Ô∏è  No {group_name} user data found (possibly no users in this range)\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Check for alpha statistics in hybrid model results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç ALPHA VALUES (Fusion Weights)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "hybrid_folders = [f for f in glob.glob('results/hybrid_*') if os.path.isdir(f)]\n",
    "\n",
    "if not hybrid_folders:\n",
    "    print(\"‚ö†Ô∏è  No hybrid model results found. Alpha tracking only works for hybrid models.\")\n",
    "else:\n",
    "    for folder in hybrid_folders:\n",
    "        alpha_path = os.path.join(folder, 'alpha_stats.json')\n",
    "        if os.path.exists(alpha_path):\n",
    "            with open(alpha_path, 'r') as f:\n",
    "                alpha_stats = json.load(f)\n",
    "            \n",
    "            folder_name = os.path.basename(folder)\n",
    "            model_name = '_'.join(folder_name.split('_')[:-2])\n",
    "            \n",
    "            print(f\"{model_name.upper()}:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for group in ['short', 'medium', 'long', 'overall']:\n",
    "                if group in alpha_stats:\n",
    "                    stats = alpha_stats[group]\n",
    "                    if group != 'overall' and 'count' in stats:\n",
    "                        print(f\"  {group.capitalize():8s}: mean={stats['mean']:.3f}, std={stats['std']:.3f}, count={stats['count']}\")\n",
    "                    elif group == 'overall':\n",
    "                        print(f\"  {group.capitalize():8s}: mean={stats['mean']:.3f}, std={stats['std']:.3f}\")\n",
    "            print()\n",
    "        else:\n",
    "            # Show expected alpha values based on model type\n",
    "            folder_name = os.path.basename(folder)\n",
    "            model_name = '_'.join(folder_name.split('_')[:-2])\n",
    "            \n",
    "            print(f\"{model_name.upper()}:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            if 'discrete' in model_name:\n",
    "                print(\"  Expected: Short=0.3, Medium=0.5, Long=0.7 (discrete bins)\")\n",
    "            elif 'fixed' in model_name:\n",
    "                print(\"  Expected: All users = 0.5 (fixed fusion)\")\n",
    "            elif 'learnable' in model_name:\n",
    "                print(\"  Expected: Learned during training (check model params)\")\n",
    "            elif 'continuous' in model_name:\n",
    "                print(\"  Expected: Smooth function of sequence length\")\n",
    "            \n",
    "            print(\"  ‚ö†Ô∏è  Alpha statistics not saved (enable with track_alpha=True)\")\n",
    "            print()\n",
    "    \n",
    "    print(\"\\nüí° Alpha interpretation:\")\n",
    "    print(\"   ‚Ä¢ Œ± close to 0: More weight on GNN (collaborative)\")\n",
    "    print(\"   ‚Ä¢ Œ± close to 1: More weight on SASRec (sequential)\")\n",
    "    print(\"   ‚Ä¢ Œ± = 0.5: Equal balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086cb30",
   "metadata": {},
   "source": [
    "## Step 10b: Alpha Statistics (Hybrid Models Only)\n",
    "\n",
    "For hybrid models, check what fusion weights (alpha values) were used for different user groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2feda3",
   "metadata": {},
   "source": [
    "## Step 11: Visualize Learning Curves\n",
    "\n",
    "Plot training loss and validation NDCG over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Find all experiment results\n",
    "result_folders = glob.glob('results/*_*')\n",
    "\n",
    "if len(result_folders) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    for folder in result_folders:\n",
    "        history_path = os.path.join(folder, 'history.json')\n",
    "        if os.path.exists(history_path):\n",
    "            try:\n",
    "                with open(history_path, 'r') as f:\n",
    "                    history = json.load(f)\n",
    "                \n",
    "                # Extract model name from folder\n",
    "                parts = os.path.basename(folder).split('_')\n",
    "                model_name = '_'.join(parts[:-2]) if len(parts) > 2 else parts[0]\n",
    "                \n",
    "                if 'train_loss' in history and history['train_loss']:\n",
    "                    ax1.plot(history['train_loss'], label=model_name, marker='o', markersize=3, linewidth=2)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not load history from {folder}: {e}\")\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('BPR Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Validation NDCG@10\n",
    "    for folder in result_folders:\n",
    "        history_path = os.path.join(folder, 'history.json')\n",
    "        if os.path.exists(history_path):\n",
    "            try:\n",
    "                with open(history_path, 'r') as f:\n",
    "                    history = json.load(f)\n",
    "                \n",
    "                parts = os.path.basename(folder).split('_')\n",
    "                model_name = '_'.join(parts[:-2]) if len(parts) > 2 else parts[0]\n",
    "                \n",
    "                if 'val_metrics' in history and history['val_metrics']:\n",
    "                    ndcg_values = [m.get('NDCG@10', 0) for m in history['val_metrics']]\n",
    "                    if ndcg_values:\n",
    "                        ax2.plot(ndcg_values, label=model_name, marker='o', markersize=3, linewidth=2)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not load validation metrics from {folder}: {e}\")\n",
    "    \n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('NDCG@10', fontsize=12)\n",
    "    ax2.set_title('Validation NDCG@10', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    plt.savefig('results/learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Saved to: results/learning_curves.png\")\n",
    "else:\n",
    "    print(\"No results to plot. Run experiments first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1299e15",
   "metadata": {},
   "source": [
    "## Step 12: Download Results\n",
    "\n",
    "Create a zip file of all results for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip of all results\n",
    "import os\n",
    "\n",
    "if os.path.exists('results') and os.listdir('results'):\n",
    "    !zip -r results.zip results/\n",
    "    \n",
    "    print(\"\\n‚úÖ Success!\")\n",
    "    print(\"Download 'results.zip' from the Output tab (right sidebar) ‚Üí\")\n",
    "    print(\"\\nContains:\")\n",
    "    print(\"  ‚Ä¢ Model checkpoints (best_model.pt)\")\n",
    "    print(\"  ‚Ä¢ Training history (history.json)\")\n",
    "    print(\"  ‚Ä¢ Test metrics (results.json)\")\n",
    "    print(\"  ‚Ä¢ Comparison tables (CSV files, if generated)\")\n",
    "    print(\"  ‚Ä¢ Learning curves (PNG)\")\n",
    "    \n",
    "    # Show what's in results\n",
    "    result_folders = [d for d in os.listdir('results') if os.path.isdir(os.path.join('results', d))]\n",
    "    print(f\"\\nüì¶ Packaged {len(result_folders)} experiment(s):\")\n",
    "    for folder in result_folders:\n",
    "        print(f\"  ‚Ä¢ {folder}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results folder found. Run experiments first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67291a1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "You've successfully:\n",
    "1. ‚úÖ Cloned repository with preprocessed data\n",
    "2. ‚úÖ Installed all dependencies\n",
    "3. ‚úÖ Verified GPU availability\n",
    "4. ‚úÖ Tested training pipeline\n",
    "5. ‚úÖ Trained recommendation models\n",
    "6. ‚úÖ Analyzed and compared results\n",
    "7. ‚úÖ Visualized learning curves\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Key Results\n",
    "\n",
    "**Dataset:** MovieLens-1M\n",
    "- 6,034 users\n",
    "- 3,533 items  \n",
    "- 1M+ ratings\n",
    "- 151,874 co-occurrence edges\n",
    "\n",
    "**Models Trained:**\n",
    "- SASRec (Transformer baseline)\n",
    "- Hybrid with Discrete Fusion (length-adaptive)\n",
    "\n",
    "**Metrics:** Hit Rate (HR), NDCG, MRR at K={5, 10, 20}\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Training Time & Epochs FAQ\n",
    "\n",
    "**Q: Why 50 epochs instead of 600 like in papers?**\n",
    "\n",
    "**A:** We use **early stopping** (patience=10):\n",
    "- Training automatically stops when validation NDCG@10 stops improving\n",
    "- With 50 epochs max ‚Üí usually converges at epoch 20-30 (~8-10 min GPU)\n",
    "- With 600 epochs max ‚Üí usually converges at epoch 30-40 (~35-45 min GPU)\n",
    "- Performance difference: ~2-3% for 10x more training time\n",
    "\n",
    "**Default (Fast):**\n",
    "```bash\n",
    "--epochs 50 --patience 10  # 8-10 min GPU, 95-98% of max performance\n",
    "```\n",
    "\n",
    "**Paper Setting (Thorough):**\n",
    "```bash\n",
    "--epochs 600 --patience 20  # 35-45 min GPU with early stopping, 100% performance\n",
    "```\n",
    "\n",
    "**Without Early Stopping (Not Recommended):**\n",
    "```bash\n",
    "--epochs 600 --patience 9999  # 80+ min GPU, risk of overfitting\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "**1. Match paper settings (600 epochs):**\n",
    "```python\n",
    "!python experiments/run_experiment.py \\\n",
    "    --model hybrid_discrete \\\n",
    "    --epochs 600 \\\n",
    "    --patience 20 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.001\n",
    "```\n",
    "\n",
    "**2. Experiment with hyperparameters:**\n",
    "```python\n",
    "!python experiments/run_experiment.py \\\n",
    "    --model hybrid_discrete \\\n",
    "    --epochs 100 \\\n",
    "    --batch_size 512 \\\n",
    "    --lr 0.0005 \\\n",
    "    --d_model 128 \\\n",
    "    --n_heads 4\n",
    "```\n",
    "\n",
    "**3. Try different fusion strategies:**\n",
    "- `--model hybrid_learnable` - Per-user learned weights\n",
    "- `--model hybrid_continuous` - Neural network fusion\n",
    "- `--model hybrid_fixed` - Fixed alpha=0.5\n",
    "\n",
    "**4. Analyze specific user groups:**\n",
    "Check `results/comparison_*.csv` for performance on short/medium/long history users\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **GitHub:** https://github.com/faroukq1/length-adaptive\n",
    "- **Paper:** Length-Adaptive Hybrid Sequential Recommendation\n",
    "- **Dataset:** MovieLens-1M (GroupLens)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the README.md and EXPERIMENTS.md in the repository."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
