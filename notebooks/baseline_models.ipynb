{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Baseline Sequential Recommendation Models — Paper Experiments\n",
    "\n",
    "**Dataset:** MovieLens-1M &nbsp;|&nbsp; **Evaluation:** HR@K · NDCG@K · MRR@K &nbsp;|&nbsp; K ∈ {5, 10, 20}\n",
    "\n",
    "---\n",
    "\n",
    "## Models & Paper-Accurate Hyperparameters\n",
    "\n",
    "| Model | Paper | d | heads | layers | max_len | dropout | Notes |\n",
    "|-------|-------|---|-------|--------|---------|---------|-------|\n",
    "| **GRU4Rec** | Hidasi et al., RecSys 2016 | 64 | — | 1 GRU | 50 | 0.1 | — |\n",
    "| **SASRec** | Kang & McAuley, ICDM 2018 | 50 | 1 | 2 blocks | 200 | 0.2 | d_ff=200 |\n",
    "| **BERT4Rec** | Sun et al., CIKM 2019 | 64 | 2 | 2 blocks | 200 | 0.2 | d_ff=256 |\n",
    "| **LightGCN** | He et al., SIGIR 2020 | 64 | — | 3 GCN | 50 | 0.0 | wd=1e-4 |\n",
    "| **Caser** | Tang & Wang, WSDM 2018 | 50 | — | — | 50 | 0.5 | L=5, nh=16, nv=4 |\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start\n",
    "1. Run **Step 1** to set up the environment\n",
    "2. Run **Step 2** to verify / prepare data\n",
    "3. Run **Steps 3–7** to train each model *(~1–2 h per model on GPU)*\n",
    "4. Run **Steps 8–10** to collect results, plot charts, and export LaTeX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1_md",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step1_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# Make sure we are at the project root\n",
    "PROJECT_ROOT = os.path.abspath('..')\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import torch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('=' * 60)\n",
    "print('ENVIRONMENT')\n",
    "print('=' * 60)\n",
    "print(f'  Working dir  : {os.getcwd()}')\n",
    "print(f'  Device       : {DEVICE}')\n",
    "if DEVICE == 'cuda':\n",
    "    print(f'  GPU          : {torch.cuda.get_device_name(0)}')\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'  GPU Memory   : {mem_gb:.1f} GB')\n",
    "print(f'  PyTorch      : {torch.__version__}')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('Training settings (paper-level):')\n",
    "print('  Max epochs : 200  (with early stopping patience=20)')\n",
    "print('  Batch size : 256  (128 for SASRec)')\n",
    "print('  LR         : 0.001')\n",
    "print('  Seed       : 42')\n",
    "print()\n",
    "print('Models to train:')\n",
    "print('  [1] GRU4Rec  -- Hidasi et al., RecSys 2016')\n",
    "print('  [2] SASRec   -- Kang & McAuley, ICDM 2018')\n",
    "print('  [3] BERT4Rec -- Sun et al., CIKM 2019')\n",
    "print('  [4] LightGCN -- He et al., SIGIR 2020')\n",
    "print('  [5] Caser    -- Tang & Wang, WSDM 2018')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2_md",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Data\n",
    "\n",
    "Checks for preprocessed MovieLens-1M files. Downloads and preprocesses automatically if missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, numpy as np\n",
    "\n",
    "data_file  = 'data/ml-1m/processed/sequences.pkl'\n",
    "graph_file = 'data/graphs/cooccurrence_graph.pkl'\n",
    "raw_file   = 'data/ml-1m/raw/ml-1m/ratings.dat'\n",
    "\n",
    "print('=' * 70)\n",
    "print('Checking Data Files')\n",
    "print('=' * 70)\n",
    "\n",
    "files_ok = True\n",
    "for path, label in [(data_file, 'Sequences (pkl)'), (graph_file, 'Graph (pkl)'), (raw_file, 'Raw ratings')]:\n",
    "    if os.path.exists(path):\n",
    "        mb = os.path.getsize(path) / 1024 / 1024\n",
    "        print(f'  OK  {label:20s}: {path}  ({mb:.2f} MB)')\n",
    "    else:\n",
    "        print(f'  MISSING  {label:20s}: {path}')\n",
    "        files_ok = False\n",
    "\n",
    "if not files_ok:\n",
    "    print()\n",
    "    print('Running preprocessing...')\n",
    "    if not os.path.exists(raw_file):\n",
    "        print('Downloading MovieLens-1M...')\n",
    "        !mkdir -p data/ml-1m/raw\n",
    "        !wget -q http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "        !unzip -q ml-1m.zip -d data/ml-1m/raw/\n",
    "        !rm -f ml-1m.zip\n",
    "    !python -m src.data.preprocess\n",
    "    !python -m src.data.graph_builder\n",
    "    print('Preprocessing complete!')\n",
    "else:\n",
    "    print()\n",
    "    print('All data files ready!')\n",
    "print('=' * 70)\n",
    "\n",
    "# Print dataset statistics for paper\n",
    "with open(data_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "cfg         = data['config']\n",
    "seq_lengths = [len(s) for s in data['train_sequences'].values()]\n",
    "\n",
    "short_u = sum(1 for l in seq_lengths if l <= 10)\n",
    "long_u  = sum(1 for l in seq_lengths if l > 50)\n",
    "mid_u   = cfg['num_users'] - short_u - long_u\n",
    "\n",
    "print()\n",
    "print('=== DATASET STATISTICS (for paper) ===')\n",
    "print(f'  Dataset       : MovieLens-1M')\n",
    "print(f'  Filtering     : rating >= 4 (implicit positive), min_seq_len = 5')\n",
    "print(f'  Split         : leave-one-out (val=second-last, test=last)')\n",
    "print(f'  Users         : {cfg[\"num_users\"]:,}')\n",
    "print(f'  Items         : {cfg[\"num_items\"]:,}')\n",
    "print(f'  Train seqs    : {len(data[\"train_sequences\"]):,}')\n",
    "print(f'  Val instances : {len(data[\"val_data\"]):,}')\n",
    "print(f'  Test instances: {len(data[\"test_data\"]):,}')\n",
    "print(f'  Seq len avg   : {np.mean(seq_lengths):.1f}')\n",
    "print(f'  Seq len median: {np.median(seq_lengths):.1f}')\n",
    "print(f'  Seq len min   : {min(seq_lengths)}')\n",
    "print(f'  Seq len max   : {max(seq_lengths)}')\n",
    "print()\n",
    "print('  User groups (based on training sequence length):')\n",
    "print(f'    Short  (len <= 10) : {short_u:,}  ({100*short_u/cfg[\"num_users\"]:.1f}%)')\n",
    "print(f'    Medium (11 <= 50)  : {mid_u:,}  ({100*mid_u/cfg[\"num_users\"]:.1f}%)')\n",
    "print(f'    Long   (len >  50) : {long_u:,}  ({100*long_u/cfg[\"num_users\"]:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_md",
   "metadata": {},
   "source": [
    "## Step 3: Train GRU4Rec\n",
    "\n",
    "> Hidasi, B., Karatzoglou, A., Baltrunas, L., & Tikk, D. (2016). *Session-based Recommendations with Recurrent Neural Networks.* ICLR.\n",
    "\n",
    "**Paper config (ML-1M):** d=64, n_layers=1, dropout=0.1, batch=256, lr=0.001, epochs=200 (early stop patience=20)\n",
    "\n",
    "Expected time: ~30–60 min (GPU) | ~4–6 h (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3_code",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python -m experiments.run_experiment \\\n",
    "    --model gru4rec \\\n",
    "    --d_model 64 \\\n",
    "    --n_layers 1 \\\n",
    "    --dropout 0.1 \\\n",
    "    --max_len 50 \\\n",
    "    --epochs 200 \\\n",
    "    --patience 20 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.001 \\\n",
    "    --weight_decay 0.0 \\\n",
    "    --num_workers 0 \\\n",
    "    --seed 42\n",
    "\n",
    "print()\n",
    "print('GRU4Rec training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4_md",
   "metadata": {},
   "source": [
    "## Step 4: Train SASRec\n",
    "\n",
    "> Kang, W., & McAuley, J. (2018). *Self-Attentive Sequential Recommendation.* ICDM.\n",
    "\n",
    "**Paper config (ML-1M):** d=50, n_heads=1, n_blocks=2, d_ff=200, max_len=200, dropout=0.2, batch=128, lr=0.001\n",
    "\n",
    "Expected time: ~60–90 min (GPU) | ~5–8 h (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4_code",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python -m experiments.run_experiment \\\n",
    "    --model sasrec \\\n",
    "    --d_model 50 \\\n",
    "    --n_heads 1 \\\n",
    "    --n_blocks 2 \\\n",
    "    --d_ff 200 \\\n",
    "    --max_len 200 \\\n",
    "    --dropout 0.2 \\\n",
    "    --epochs 200 \\\n",
    "    --patience 20 \\\n",
    "    --batch_size 128 \\\n",
    "    --lr 0.001 \\\n",
    "    --weight_decay 0.0 \\\n",
    "    --num_workers 0 \\\n",
    "    --seed 42\n",
    "\n",
    "print()\n",
    "print('SASRec training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5_md",
   "metadata": {},
   "source": [
    "## Step 5: Train BERT4Rec\n",
    "\n",
    "> Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., & Jiang, P. (2019). *BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer.* CIKM.\n",
    "\n",
    "**Paper config (ML-1M):** d=64, n_heads=2, n_blocks=2, d_ff=256, max_len=200, dropout=0.2, batch=256, lr=0.001\n",
    "\n",
    "Expected time: ~60–90 min (GPU) | ~5–8 h (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step5_code",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python -m experiments.run_experiment \\\n",
    "    --model bert4rec \\\n",
    "    --d_model 64 \\\n",
    "    --n_heads 2 \\\n",
    "    --n_blocks 2 \\\n",
    "    --d_ff 256 \\\n",
    "    --max_len 200 \\\n",
    "    --dropout 0.2 \\\n",
    "    --epochs 200 \\\n",
    "    --patience 20 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.001 \\\n",
    "    --weight_decay 0.0 \\\n",
    "    --num_workers 0 \\\n",
    "    --seed 42\n",
    "\n",
    "print()\n",
    "print('BERT4Rec training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6_md",
   "metadata": {},
   "source": [
    "## Step 6: Train LightGCN\n",
    "\n",
    "> He, X., Deng, K., Wang, X., Li, Y., Zhang, Y., & Wang, M. (2020). *LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation.* SIGIR.\n",
    "\n",
    "**Paper config (ML-1M):** d=64, gnn_layers=3, no dropout, weight_decay=1e-4, batch=256, lr=0.001\n",
    "\n",
    "Expected time: ~30–60 min (GPU) | ~3–5 h (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6_code",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python -m experiments.run_experiment \\\n",
    "    --model lightgcn \\\n",
    "    --d_model 64 \\\n",
    "    --gnn_layers 3 \\\n",
    "    --dropout 0.0 \\\n",
    "    --max_len 50 \\\n",
    "    --epochs 200 \\\n",
    "    --patience 20 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.001 \\\n",
    "    --weight_decay 1e-4 \\\n",
    "    --num_workers 0 \\\n",
    "    --seed 42\n",
    "\n",
    "print()\n",
    "print('LightGCN training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7_md",
   "metadata": {},
   "source": [
    "## Step 7: Train Caser\n",
    "\n",
    "> Tang, J., & Wang, K. (2018). *Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding.* WSDM.\n",
    "\n",
    "**Paper config (ML-1M):** d=50, L=5 (window), nh=16 (horizontal filters), nv=4 (vertical filters), dropout=0.5, batch=256, lr=0.001\n",
    "\n",
    "Expected time: ~30–60 min (GPU) | ~3–5 h (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step7_code",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python -m experiments.run_experiment \\\n",
    "    --model caser \\\n",
    "    --d_model 50 \\\n",
    "    --L_caser 5 \\\n",
    "    --nh 16 \\\n",
    "    --nv 4 \\\n",
    "    --dropout 0.5 \\\n",
    "    --max_len 50 \\\n",
    "    --epochs 200 \\\n",
    "    --patience 20 \\\n",
    "    --batch_size 256 \\\n",
    "    --lr 0.001 \\\n",
    "    --weight_decay 1e-4 \\\n",
    "    --num_workers 0 \\\n",
    "    --seed 42\n",
    "\n",
    "print()\n",
    "print('Caser training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8_md",
   "metadata": {},
   "source": [
    "## Step 8: Collect & Compare Results\n",
    "\n",
    "Reads all `results/*/results.json` files produced by Steps 3–7 and builds a comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step8_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, json, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RESULTS_DIR  = 'results'\n",
    "MODEL_MAP    = {'gru4rec': 'GRU4Rec', 'sasrec': 'SASRec',\n",
    "                'bert4rec': 'BERT4Rec', 'lightgcn': 'LightGCN', 'caser': 'Caser'}\n",
    "PAPER_ORDER  = ['GRU4Rec', 'SASRec', 'BERT4Rec', 'LightGCN', 'Caser']\n",
    "K_LIST       = [5, 10, 20]\n",
    "\n",
    "# Collect latest run per baseline model\n",
    "latest = {}  # model_key -> (exp_dir, mtime)\n",
    "for exp_dir in glob.glob(os.path.join(RESULTS_DIR, '*')):\n",
    "    cfg_path = os.path.join(exp_dir, 'config.json')\n",
    "    res_path = os.path.join(exp_dir, 'results.json')\n",
    "    if not (os.path.exists(cfg_path) and os.path.exists(res_path)):\n",
    "        continue\n",
    "    with open(cfg_path) as f:\n",
    "        cfg = json.load(f)\n",
    "    model_key = cfg.get('model', '')\n",
    "    if model_key not in MODEL_MAP:\n",
    "        continue\n",
    "    mtime = os.path.getmtime(res_path)\n",
    "    if model_key not in latest or mtime > latest[model_key][1]:\n",
    "        latest[model_key] = (exp_dir, mtime)\n",
    "\n",
    "if not latest:\n",
    "    print('No baseline results found. Run Steps 3-7 first.')\n",
    "else:\n",
    "    rows = []\n",
    "    for model_key, (exp_dir, _) in latest.items():\n",
    "        with open(os.path.join(exp_dir, 'config.json'))  as f: cfg = json.load(f)\n",
    "        with open(os.path.join(exp_dir, 'results.json')) as f: res = json.load(f)\n",
    "        m    = res['test_metrics']\n",
    "        name = MODEL_MAP[model_key]\n",
    "        row  = {'Model': name, 'Best Epoch': res.get('best_epoch', '-'),\n",
    "                'Best Val NDCG@10': round(res.get('best_val_metric', 0), 4)}\n",
    "        for k in K_LIST:\n",
    "            row[f'HR@{k}']   = round(m.get(f'HR@{k}',   0), 4)\n",
    "            row[f'NDCG@{k}'] = round(m.get(f'NDCG@{k}', 0), 4)\n",
    "            row[f'MRR@{k}']  = round(m.get(f'MRR@{k}',  0), 4)\n",
    "        rows.append(row)\n",
    "\n",
    "    df = (pd.DataFrame(rows)\n",
    "            .set_index('Model')\n",
    "            .reindex([m for m in PAPER_ORDER if m in [r['Model'] for r in rows]]))\n",
    "\n",
    "    print('=' * 80)\n",
    "    print('OVERALL TEST RESULTS  (MovieLens-1M, leave-one-out evaluation)')\n",
    "    print('=' * 80)\n",
    "    print(df.to_string())\n",
    "    print()\n",
    "    print(f'Best HR@10  : {df[\"HR@10\"].idxmax()}  ({df[\"HR@10\"].max():.4f})')\n",
    "    print(f'Best NDCG@10: {df[\"NDCG@10\"].idxmax()}  ({df[\"NDCG@10\"].max():.4f})')\n",
    "    print(f'Best MRR@10 : {df[\"MRR@10\"].idxmax()}  ({df[\"MRR@10\"].max():.4f})')\n",
    "    df.to_csv('baseline_overall_results.csv')\n",
    "    print()\n",
    "    print('Saved: baseline_overall_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step8b_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results by user group (short / medium / long)\n",
    "if latest:\n",
    "    group_rows = []\n",
    "    for model_key, (exp_dir, _) in latest.items():\n",
    "        with open(os.path.join(exp_dir, 'results.json')) as f:\n",
    "            res = json.load(f)\n",
    "        name = MODEL_MAP[model_key]\n",
    "        for group, gm in res.get('grouped_metrics', {}).items():\n",
    "            r = {'Model': name, 'Group': group}\n",
    "            for k in K_LIST:\n",
    "                r[f'HR@{k}']   = round(gm.get(f'HR@{k}',   0), 4)\n",
    "                r[f'NDCG@{k}'] = round(gm.get(f'NDCG@{k}', 0), 4)\n",
    "                r[f'MRR@{k}']  = round(gm.get(f'MRR@{k}',  0), 4)\n",
    "            r['Count'] = gm.get('count', '-')\n",
    "            group_rows.append(r)\n",
    "\n",
    "    if group_rows:\n",
    "        df_grp = pd.DataFrame(group_rows)\n",
    "        pivot = (df_grp.pivot_table(index='Model', columns='Group', values='NDCG@10')\n",
    "                       .reindex([m for m in PAPER_ORDER if m in df_grp['Model'].values]))\n",
    "        # Reorder columns\n",
    "        col_order = [c for c in ['short','medium','long','overall'] if c in pivot.columns]\n",
    "        pivot = pivot[col_order]\n",
    "        print('=' * 60)\n",
    "        print('NDCG@10 BY USER GROUP')\n",
    "        print('=' * 60)\n",
    "        print(pivot.to_string())\n",
    "        df_grp.to_csv('baseline_grouped_results.csv', index=False)\n",
    "        print()\n",
    "        print('Saved: baseline_grouped_results.csv')\n",
    "    else:\n",
    "        print('No grouped metrics found in results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9_md",
   "metadata": {},
   "source": [
    "## Step 9: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step9a_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if latest:\n",
    "    model_names  = [m for m in PAPER_ORDER if m in df.index]\n",
    "    metrics_sets = [\n",
    "        ('Hit Rate @ K',  'HR@5',   'HR@10',   'HR@20'),\n",
    "        ('NDCG @ K',      'NDCG@5', 'NDCG@10', 'NDCG@20'),\n",
    "        ('MRR @ K',       'MRR@5',  'MRR@10',  'MRR@20'),\n",
    "    ]\n",
    "    colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "    x      = np.arange(len(model_names))\n",
    "    width  = 0.22\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    for ax, (title, m5, m10, m20) in zip(axes, metrics_sets):\n",
    "        for i, (col, lbl, clr) in enumerate([(m5,'@5',colors[0]),(m10,'@10',colors[1]),(m20,'@20',colors[2])]):\n",
    "            vals = [df.loc[m, col] if m in df.index else 0 for m in model_names]\n",
    "            ax.bar(x + (i-1)*width, vals, width, label=lbl, color=clr, edgecolor='white')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.set_ylim(bottom=0)\n",
    "\n",
    "    plt.suptitle('Baseline Model Comparison on MovieLens-1M', fontsize=14, fontweight='bold', y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('baseline_metric_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved: baseline_metric_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step9b_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Heatmap: NDCG@10 by user group\n",
    "if latest and group_rows:\n",
    "    col_order = [c for c in ['short','medium','long','overall'] if c in pivot.columns]\n",
    "    heat = pivot[col_order].copy()\n",
    "    heat.columns = [c.capitalize() for c in heat.columns]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    sns.heatmap(heat, annot=True, fmt='.4f', cmap='YlOrRd',\n",
    "                linewidths=0.5, cbar_kws={'label': 'NDCG@10'}, ax=ax)\n",
    "    ax.set_title('NDCG@10 by User Group — MovieLens-1M', fontweight='bold')\n",
    "    ax.set_xlabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('baseline_group_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved: baseline_group_heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step10_md",
   "metadata": {},
   "source": [
    "## Step 10: Paper-Ready LaTeX Table\n",
    "\n",
    "Copy the output directly into your paper (requires `booktabs` package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step10_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest:\n",
    "    key_cols = ['HR@5','NDCG@5','MRR@5','HR@10','NDCG@10','MRR@10','HR@20','NDCG@20','MRR@20']\n",
    "    df_paper = df[key_cols]\n",
    "\n",
    "    col_fmt = 'l' + 'r' * len(key_cols)\n",
    "    header  = 'Model & ' + ' & '.join(key_cols) + ' \\\\\\\\'\n",
    "\n",
    "    lines = [\n",
    "        '\\\\begin{table}[ht]',\n",
    "        '  \\\\centering',\n",
    "        '  \\\\caption{Performance comparison of baseline sequential recommendation models on MovieLens-1M.}',\n",
    "        '  \\\\label{tab:baselines}',\n",
    "        f'  \\\\begin{{tabular}}{{{col_fmt}}}',\n",
    "        '    \\\\toprule',\n",
    "        f'    {header}',\n",
    "        '    \\\\midrule',\n",
    "    ]\n",
    "\n",
    "    for idx, row in df_paper.iterrows():\n",
    "        cells = []\n",
    "        for col, v in row.items():\n",
    "            # Bold the best value in each column\n",
    "            if v == df_paper[col].max():\n",
    "                cells.append(f'\\\\textbf{{{v:.4f}}}')\n",
    "            else:\n",
    "                cells.append(f'{v:.4f}')\n",
    "        lines.append(f'    {idx} & ' + ' & '.join(cells) + ' \\\\\\\\')\n",
    "\n",
    "    lines += ['    \\\\bottomrule', '  \\\\end{tabular}', '\\\\end{table}']\n",
    "    latex = '\\n'.join(lines)\n",
    "\n",
    "    print(latex)\n",
    "    with open('baseline_latex_table.tex', 'w') as f:\n",
    "        f.write(latex)\n",
    "    print()\n",
    "    print('Saved: baseline_latex_table.tex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_md",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All generated files:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `baseline_overall_results.csv` | Full metrics table (HR / NDCG / MRR @ 5, 10, 20) |\n",
    "| `baseline_grouped_results.csv` | Metrics per user group (short / medium / long / overall) |\n",
    "| `baseline_latex_table.tex` | LaTeX `\\begin{table}` ready to paste in paper |\n",
    "| `baseline_metric_comparison.png` | Bar chart HR/NDCG/MRR @ K for all models |\n",
    "| `baseline_group_heatmap.png` | NDCG@10 heatmap by user group |\n",
    "| `results/<model>_<timestamp>/` | Per-model checkpoint, config, history & results JSON |\n"
   ]
  }
 ]
}