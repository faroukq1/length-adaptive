{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a1b2c3d4",
            "metadata": {},
            "source": [
                "# Baseline Sequential Recommendation Models -- Paper Experiments\n",
                "\n",
                "**Dataset:** MovieLens-1M  |  **Evaluation:** HR@K, NDCG@K, MRR@K  |  K in {5, 10, 20}\n",
                "\n",
                "---\n",
                "\n",
                "## Models and Paper-Accurate Hyperparameters\n",
                "\n",
                "| Model | Paper | d | heads | layers | max_len | dropout | Notes |\n",
                "|-------|-------|---|-------|--------|---------|---------|-------|\n",
                "| **GRU4Rec** | Hidasi et al., RecSys 2016 | 64 | - | 1 GRU | 50 | 0.1 | - |\n",
                "| **SASRec** | Kang and McAuley, ICDM 2018 | 50 | 1 | 2 blocks | 200 | 0.2 | d_ff=200 |\n",
                "| **BERT4Rec** | Sun et al., CIKM 2019 | 64 | 2 | 2 blocks | 200 | 0.2 | d_ff=256 |\n",
                "| **LightGCN** | He et al., SIGIR 2020 | 64 | - | 3 GCN | 50 | 0.0 | wd=1e-4 |\n",
                "| **Caser** | Tang and Wang, WSDM 2018 | 50 | - | - | 50 | 0.5 | L=5, nh=16, nv=4 |\n",
                "\n",
                "---\n",
                "\n",
                "## Quick Start\n",
                "1. Run **Step 1** to clone repo and install deps\n",
                "2. Run **Step 2** to verify / prepare data\n",
                "3. Run **Steps 3-7** to train each model (~1-2 h per model on GPU)\n",
                "4. Run **Steps 8-10** to collect results, plot charts, and export LaTeX\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step1_md",
            "metadata": {},
            "source": [
                "## Step 1: Clone Repository and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step1_code",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Clone the repo if not already present\n",
                "if not os.path.exists('length-adaptive'):\n",
                "    !git clone https://github.com/faroukq1/length-adaptive.git\n",
                "    print('Repository cloned.')\n",
                "else:\n",
                "    print('Repository already exists.')\n",
                "\n",
                "# Change into the project root\n",
                "%cd length-adaptive\n",
                "\n",
                "# Install dependencies\n",
                "!pip install -q torch-geometric tqdm scikit-learn pandas matplotlib seaborn\n",
                "\n",
                "import torch\n",
                "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "\n",
                "print()\n",
                "print('=' * 60)\n",
                "print('ENVIRONMENT')\n",
                "print('=' * 60)\n",
                "print(f'  Working dir  : {os.getcwd()}')\n",
                "print(f'  Device       : {DEVICE}')\n",
                "if DEVICE == 'cuda':\n",
                "    print(f'  GPU          : {torch.cuda.get_device_name(0)}')\n",
                "    mem_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
                "    print(f'  GPU Memory   : {mem_gb:.1f} GB')\n",
                "print(f'  PyTorch      : {torch.__version__}')\n",
                "print('=' * 60)\n",
                "print()\n",
                "print('Training settings (paper-level):')\n",
                "print('  Max epochs : 200  (early stopping patience=20)')\n",
                "print('  Batch size : 256  (128 for SASRec)')\n",
                "print('  LR         : 0.001')\n",
                "print('  Seed       : 42')\n",
                "print()\n",
                "print('Models:')\n",
                "print('  [1] GRU4Rec  -- Hidasi et al., RecSys 2016')\n",
                "print('  [2] SASRec   -- Kang & McAuley, ICDM 2018')\n",
                "print('  [3] BERT4Rec -- Sun et al., CIKM 2019')\n",
                "print('  [4] LightGCN -- He et al., SIGIR 2020')\n",
                "print('  [5] Caser    -- Tang & Wang, WSDM 2018')\n",
                "print('=' * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step2_md",
            "metadata": {},
            "source": [
                "## Step 2: Prepare Data\n",
                "\n",
                "Checks for preprocessed MovieLens-1M files. Downloads and preprocesses automatically if missing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step2_code",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import os, pickle, numpy as np\n",
                "\n",
                "data_file  = 'data/ml-1m/processed/sequences.pkl'\n",
                "graph_file = 'data/graphs/cooccurrence_graph.pkl'\n",
                "raw_file   = 'data/ml-1m/raw/ml-1m/ratings.dat'\n",
                "\n",
                "print('=' * 70)\n",
                "print('Checking Data Files')\n",
                "print('=' * 70)\n",
                "\n",
                "files_ok = True\n",
                "for path, label in [(data_file, 'Sequences'), (graph_file, 'Graph'), (raw_file, 'Raw ratings')]:\n",
                "    if os.path.exists(path):\n",
                "        mb = os.path.getsize(path) / 1024 / 1024\n",
                "        print(f'  OK      {label:15s}: {path}  ({mb:.2f} MB)')\n",
                "    else:\n",
                "        print(f'  MISSING {label:15s}: {path}')\n",
                "        files_ok = False\n",
                "\n",
                "if not files_ok:\n",
                "    print()\n",
                "    print('Running preprocessing...')\n",
                "    if not os.path.exists(raw_file):\n",
                "        print('Downloading MovieLens-1M...')\n",
                "        !mkdir -p data/ml-1m/raw\n",
                "        !wget -q http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
                "        !unzip -q ml-1m.zip -d data/ml-1m/raw/\n",
                "        !rm -f ml-1m.zip\n",
                "    !python -m src.data.preprocess\n",
                "    !python -m src.data.graph_builder\n",
                "    print('Preprocessing complete!')\n",
                "else:\n",
                "    print()\n",
                "    print('All data files ready!')\n",
                "print('=' * 70)\n",
                "\n",
                "# Dataset statistics for paper\n",
                "with open(data_file, 'rb') as f:\n",
                "    data = pickle.load(f)\n",
                "\n",
                "cfg         = data['config']\n",
                "seq_lengths = [len(s) for s in data['train_sequences'].values()]\n",
                "short_u = sum(1 for l in seq_lengths if l <= 10)\n",
                "long_u  = sum(1 for l in seq_lengths if l > 50)\n",
                "mid_u   = cfg['num_users'] - short_u - long_u\n",
                "\n",
                "print()\n",
                "print('=== DATASET STATISTICS (for paper) ===')\n",
                "print(f'  Dataset       : MovieLens-1M')\n",
                "print(f'  Filtering     : rating >= 4 (implicit), min_seq_len=5')\n",
                "print(f'  Split         : leave-one-out (val=second-last, test=last)')\n",
                "print(f'  Users         : {cfg[\"num_users\"]:,}')\n",
                "print(f'  Items         : {cfg[\"num_items\"]:,}')\n",
                "print(f'  Train seqs    : {len(data[\"train_sequences\"]):,}')\n",
                "print(f'  Val instances : {len(data[\"val_data\"]):,}')\n",
                "print(f'  Test instances: {len(data[\"test_data\"]):,}')\n",
                "print(f'  Seq len avg   : {np.mean(seq_lengths):.1f}')\n",
                "print(f'  Seq len median: {np.median(seq_lengths):.1f}')\n",
                "print(f'  Seq len range : [{min(seq_lengths)}, {max(seq_lengths)}]')\n",
                "print()\n",
                "print('  User groups (training sequence length):')\n",
                "print(f'    Short  (len <= 10) : {short_u:,}  ({100*short_u/cfg[\"num_users\"]:.1f}%)')\n",
                "print(f'    Medium (11 to 50)  : {mid_u:,}  ({100*mid_u/cfg[\"num_users\"]:.1f}%)')\n",
                "print(f'    Long   (len >  50) : {long_u:,}  ({100*long_u/cfg[\"num_users\"]:.1f}%)')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step3_md",
            "metadata": {},
            "source": [
                "## Step 3: Train GRU4Rec\n",
                "\n",
                "> Hidasi, B., Karatzoglou, A., Baltrunas, L., and Tikk, D. (2016). *Session-based Recommendations with Recurrent Neural Networks.* ICLR.\n",
                "\n",
                "**Paper config:** d=64, n_layers=1, dropout=0.1, batch=256, lr=0.001"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step3_code",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "!python -m experiments.run_experiment \\\n",
                "    --model gru4rec \\\n",
                "    --d_model 64 \\\n",
                "    --n_layers 1 \\\n",
                "    --dropout 0.1 \\\n",
                "    --max_len 50 \\\n",
                "    --epochs 200 \\\n",
                "    --patience 20 \\\n",
                "    --batch_size 256 \\\n",
                "    --lr 0.001 \\\n",
                "    --weight_decay 0.0 \\\n",
                "    --num_workers 0 \\\n",
                "    --seed 42\n",
                "\n",
                "print('GRU4Rec training complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step4_md",
            "metadata": {},
            "source": [
                "## Step 4: Train SASRec\n",
                "\n",
                "> Kang, W., and McAuley, J. (2018). *Self-Attentive Sequential Recommendation.* ICDM.\n",
                "\n",
                "**Paper config:** d=50, n_heads=1, n_blocks=2, d_ff=200, max_len=200, dropout=0.2, batch=128"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step4_code",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "!python -m experiments.run_experiment \\\n",
                "    --model sasrec \\\n",
                "    --d_model 50 \\\n",
                "    --n_heads 1 \\\n",
                "    --n_blocks 2 \\\n",
                "    --d_ff 200 \\\n",
                "    --max_len 200 \\\n",
                "    --dropout 0.2 \\\n",
                "    --epochs 200 \\\n",
                "    --patience 20 \\\n",
                "    --batch_size 128 \\\n",
                "    --lr 0.001 \\\n",
                "    --weight_decay 0.0 \\\n",
                "    --num_workers 0 \\\n",
                "    --seed 42\n",
                "\n",
                "print('SASRec training complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step5_md",
            "metadata": {},
            "source": [
                "## Step 5: Train BERT4Rec\n",
                "\n",
                "> Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., and Jiang, P. (2019). *BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer.* CIKM.\n",
                "\n",
                "**Paper config:** d=64, n_heads=2, n_blocks=2, d_ff=256, max_len=200, dropout=0.2, batch=256"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step5_code",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "!python -m experiments.run_experiment \\\n",
                "    --model bert4rec \\\n",
                "    --d_model 64 \\\n",
                "    --n_heads 2 \\\n",
                "    --n_blocks 2 \\\n",
                "    --d_ff 256 \\\n",
                "    --max_len 200 \\\n",
                "    --dropout 0.2 \\\n",
                "    --epochs 200 \\\n",
                "    --patience 20 \\\n",
                "    --batch_size 256 \\\n",
                "    --lr 0.001 \\\n",
                "    --weight_decay 0.0 \\\n",
                "    --num_workers 0 \\\n",
                "    --seed 42\n",
                "\n",
                "print('BERT4Rec training complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step6_md",
            "metadata": {},
            "source": [
                "## Step 6: Train LightGCN\n",
                "\n",
                "> He, X., Deng, K., Wang, X., Li, Y., Zhang, Y., and Wang, M. (2020). *LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation.* SIGIR.\n",
                "\n",
                "**Paper config:** d=64, gnn_layers=3, dropout=0.0, weight_decay=1e-4, batch=256"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step6_code",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "!python -m experiments.run_experiment \\\n",
                "    --model lightgcn \\\n",
                "    --d_model 64 \\\n",
                "    --gnn_layers 3 \\\n",
                "    --dropout 0.0 \\\n",
                "    --max_len 50 \\\n",
                "    --epochs 200 \\\n",
                "    --patience 20 \\\n",
                "    --batch_size 256 \\\n",
                "    --lr 0.001 \\\n",
                "    --weight_decay 1e-4 \\\n",
                "    --num_workers 0 \\\n",
                "    --seed 42\n",
                "\n",
                "print('LightGCN training complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step7_md",
            "metadata": {},
            "source": [
                "## Step 7: Train Caser\n",
                "\n",
                "> Tang, J., and Wang, K. (2018). *Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding.* WSDM.\n",
                "\n",
                "**Paper config:** d=50, L=5, nh=16, nv=4, dropout=0.5, weight_decay=1e-4, batch=256"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step7_code",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": [
                "!python -m experiments.run_experiment \\\n",
                "    --model caser \\\n",
                "    --d_model 50 \\\n",
                "    --L_caser 5 \\\n",
                "    --nh 16 \\\n",
                "    --nv 4 \\\n",
                "    --dropout 0.5 \\\n",
                "    --max_len 50 \\\n",
                "    --epochs 200 \\\n",
                "    --patience 20 \\\n",
                "    --batch_size 256 \\\n",
                "    --lr 0.001 \\\n",
                "    --weight_decay 1e-4 \\\n",
                "    --num_workers 0 \\\n",
                "    --seed 42\n",
                "\n",
                "print('Caser training complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step8_md",
            "metadata": {},
            "source": [
                "## Step 8: Collect and Compare Results\n",
                "\n",
                "Reads `results/*/results.json` from Steps 3-7 and builds the comparison table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step8_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob, json, os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "RESULTS_DIR = 'results'\n",
                "MODEL_MAP   = {'gru4rec': 'GRU4Rec', 'sasrec': 'SASRec',\n",
                "               'bert4rec': 'BERT4Rec', 'lightgcn': 'LightGCN', 'caser': 'Caser'}\n",
                "PAPER_ORDER = ['GRU4Rec', 'SASRec', 'BERT4Rec', 'LightGCN', 'Caser']\n",
                "K_LIST      = [5, 10, 20]\n",
                "\n",
                "# Pick the latest result folder per baseline model\n",
                "latest = {}\n",
                "for exp_dir in sorted(glob.glob(os.path.join(RESULTS_DIR, '*'))):\n",
                "    cfg_p = os.path.join(exp_dir, 'config.json')\n",
                "    res_p = os.path.join(exp_dir, 'results.json')\n",
                "    if not (os.path.exists(cfg_p) and os.path.exists(res_p)):\n",
                "        continue\n",
                "    with open(cfg_p) as f:\n",
                "        cfg = json.load(f)\n",
                "    key = cfg.get('model', '')\n",
                "    if key not in MODEL_MAP:\n",
                "        continue\n",
                "    mtime = os.path.getmtime(res_p)\n",
                "    if key not in latest or mtime > latest[key][1]:\n",
                "        latest[key] = (exp_dir, mtime)\n",
                "\n",
                "if not latest:\n",
                "    print('No baseline results found. Run Steps 3-7 first.')\n",
                "else:\n",
                "    rows = []\n",
                "    for key, (exp_dir, _) in latest.items():\n",
                "        with open(os.path.join(exp_dir, 'config.json'))  as f: cfg = json.load(f)\n",
                "        with open(os.path.join(exp_dir, 'results.json')) as f: res = json.load(f)\n",
                "        m   = res['test_metrics']\n",
                "        row = {'Model': MODEL_MAP[key],\n",
                "               'Best Epoch': res.get('best_epoch', '-'),\n",
                "               'Val NDCG@10': round(res.get('best_val_metric', 0), 4)}\n",
                "        for k in K_LIST:\n",
                "            row[f'HR@{k}']   = round(m.get(f'HR@{k}',   0), 4)\n",
                "            row[f'NDCG@{k}'] = round(m.get(f'NDCG@{k}', 0), 4)\n",
                "            row[f'MRR@{k}']  = round(m.get(f'MRR@{k}',  0), 4)\n",
                "        rows.append(row)\n",
                "\n",
                "    df = (pd.DataFrame(rows)\n",
                "            .set_index('Model')\n",
                "            .reindex([m for m in PAPER_ORDER if m in [r['Model'] for r in rows]]))\n",
                "\n",
                "    print('=' * 80)\n",
                "    print('OVERALL TEST RESULTS  (MovieLens-1M, leave-one-out)')\n",
                "    print('=' * 80)\n",
                "    print(df.to_string())\n",
                "    print()\n",
                "    print(f'Best HR@10  : {df[\"HR@10\"].idxmax()}  ({df[\"HR@10\"].max():.4f})')\n",
                "    print(f'Best NDCG@10: {df[\"NDCG@10\"].idxmax()}  ({df[\"NDCG@10\"].max():.4f})')\n",
                "    print(f'Best MRR@10 : {df[\"MRR@10\"].idxmax()}  ({df[\"MRR@10\"].max():.4f})')\n",
                "    df.to_csv('baseline_overall_results.csv')\n",
                "    print('Saved: baseline_overall_results.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step8b_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Results by user group (short / medium / long / overall)\n",
                "if latest:\n",
                "    group_rows = []\n",
                "    for key, (exp_dir, _) in latest.items():\n",
                "        with open(os.path.join(exp_dir, 'results.json')) as f:\n",
                "            res = json.load(f)\n",
                "        for group, gm in res.get('grouped_metrics', {}).items():\n",
                "            r = {'Model': MODEL_MAP[key], 'Group': group}\n",
                "            for k in K_LIST:\n",
                "                r[f'HR@{k}']   = round(gm.get(f'HR@{k}',   0), 4)\n",
                "                r[f'NDCG@{k}'] = round(gm.get(f'NDCG@{k}', 0), 4)\n",
                "                r[f'MRR@{k}']  = round(gm.get(f'MRR@{k}',  0), 4)\n",
                "            r['Count'] = gm.get('count', '-')\n",
                "            group_rows.append(r)\n",
                "\n",
                "    if group_rows:\n",
                "        df_grp = pd.DataFrame(group_rows)\n",
                "        pivot  = (df_grp.pivot_table(index='Model', columns='Group', values='NDCG@10')\n",
                "                        .reindex([m for m in PAPER_ORDER if m in df_grp['Model'].values]))\n",
                "        col_order = [c for c in ['short','medium','long','overall'] if c in pivot.columns]\n",
                "        pivot = pivot[col_order]\n",
                "        print('=' * 60)\n",
                "        print('NDCG@10 BY USER GROUP')\n",
                "        print('=' * 60)\n",
                "        print(pivot.to_string())\n",
                "        df_grp.to_csv('baseline_grouped_results.csv', index=False)\n",
                "        print('Saved: baseline_grouped_results.csv')\n",
                "    else:\n",
                "        print('No grouped metrics found.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step9_md",
            "metadata": {},
            "source": [
                "## Step 9: Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step9a_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "if latest:\n",
                "    model_names  = [m for m in PAPER_ORDER if m in df.index]\n",
                "    metrics_sets = [\n",
                "        ('Hit Rate @ K',  'HR@5',   'HR@10',   'HR@20'),\n",
                "        ('NDCG @ K',      'NDCG@5', 'NDCG@10', 'NDCG@20'),\n",
                "        ('MRR @ K',       'MRR@5',  'MRR@10',  'MRR@20'),\n",
                "    ]\n",
                "    colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
                "    x      = np.arange(len(model_names))\n",
                "    width  = 0.22\n",
                "\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "    for ax, (title, m5, m10, m20) in zip(axes, metrics_sets):\n",
                "        for i, (col, lbl, clr) in enumerate([(m5,'@5',colors[0]),(m10,'@10',colors[1]),(m20,'@20',colors[2])]):\n",
                "            vals = [df.loc[m, col] if m in df.index else 0 for m in model_names]\n",
                "            ax.bar(x + (i-1)*width, vals, width, label=lbl, color=clr, edgecolor='white')\n",
                "        ax.set_xticks(x)\n",
                "        ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
                "        ax.set_ylabel('Score')\n",
                "        ax.set_title(title, fontweight='bold')\n",
                "        ax.legend()\n",
                "        ax.set_ylim(bottom=0)\n",
                "\n",
                "    plt.suptitle('Baseline Model Comparison -- MovieLens-1M', fontsize=14, fontweight='bold', y=1.01)\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('baseline_metric_comparison.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print('Saved: baseline_metric_comparison.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step9b_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns\n",
                "\n",
                "if latest and group_rows:\n",
                "    col_order = [c for c in ['short','medium','long','overall'] if c in pivot.columns]\n",
                "    heat = pivot[col_order].copy()\n",
                "    heat.columns = [c.capitalize() for c in heat.columns]\n",
                "\n",
                "    fig, ax = plt.subplots(figsize=(8, 4))\n",
                "    sns.heatmap(heat, annot=True, fmt='.4f', cmap='YlOrRd',\n",
                "                linewidths=0.5, cbar_kws={'label': 'NDCG@10'}, ax=ax)\n",
                "    ax.set_title('NDCG@10 by User Group -- MovieLens-1M', fontweight='bold')\n",
                "    ax.set_xlabel('')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('baseline_group_heatmap.png', dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    print('Saved: baseline_group_heatmap.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "step10_md",
            "metadata": {},
            "source": [
                "## Step 10: Paper-Ready LaTeX Table\n",
                "\n",
                "Copy the output directly into your paper (requires `booktabs` package)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "step10_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "if latest:\n",
                "    key_cols = ['HR@5','NDCG@5','MRR@5','HR@10','NDCG@10','MRR@10','HR@20','NDCG@20','MRR@20']\n",
                "    df_paper = df[key_cols]\n",
                "\n",
                "    col_fmt = 'l' + 'r' * len(key_cols)\n",
                "    header  = 'Model & ' + ' & '.join(key_cols) + ' \\\\\\\\'\n",
                "\n",
                "    lines = [\n",
                "        '\\\\begin{table}[ht]',\n",
                "        '  \\\\centering',\n",
                "        '  \\\\caption{Performance comparison of baseline sequential recommendation models on MovieLens-1M.}',\n",
                "        '  \\\\label{tab:baselines}',\n",
                "        f'  \\\\begin{{tabular}}{{{col_fmt}}}',\n",
                "        '    \\\\toprule',\n",
                "        f'    {header}',\n",
                "        '    \\\\midrule',\n",
                "    ]\n",
                "\n",
                "    for idx, row in df_paper.iterrows():\n",
                "        cells = []\n",
                "        for col, v in row.items():\n",
                "            if v == df_paper[col].max():\n",
                "                cells.append(f'\\\\textbf{{{v:.4f}}}')\n",
                "            else:\n",
                "                cells.append(f'{v:.4f}')\n",
                "        lines.append(f'    {idx} & ' + ' & '.join(cells) + ' \\\\\\\\')\n",
                "\n",
                "    lines += ['    \\\\bottomrule', '  \\\\end{tabular}', '\\\\end{table}']\n",
                "    latex = '\\n'.join(lines)\n",
                "\n",
                "    print(latex)\n",
                "    with open('baseline_latex_table.tex', 'w') as f:\n",
                "        f.write(latex)\n",
                "    print()\n",
                "    print('Saved: baseline_latex_table.tex')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary_md",
            "metadata": {},
            "source": [
                "## Summary -- Generated Artifacts\n",
                "\n",
                "| File | Description |\n",
                "|------|-------------|\n",
                "| `baseline_overall_results.csv` | Full metrics table (HR / NDCG / MRR @ 5, 10, 20) |\n",
                "| `baseline_grouped_results.csv` | Metrics per user group (short / medium / long / overall) |\n",
                "| `baseline_latex_table.tex` | LaTeX table ready to paste in paper |\n",
                "| `baseline_metric_comparison.png` | Bar chart HR/NDCG/MRR @ K for all models |\n",
                "| `baseline_group_heatmap.png` | NDCG@10 heatmap by user group |\n",
                "| `results/<model>_<timestamp>/` | Per-model checkpoint, config, history and results JSON |\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
