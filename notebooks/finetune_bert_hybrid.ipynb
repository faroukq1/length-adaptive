{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629d3a32",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT Hybrid Models\n",
    "# Hyperparameter Optimization for Best Performers\n",
    "\n",
    "**Models to Fine-Tune:**\n",
    "1. **bert_hybrid_fixed** - HR@10: 0.0690, NDCG@10: 0.1447\n",
    "2. **bert_hybrid_discrete** - HR@10: 0.0655, NDCG@10: 0.1414\n",
    "\n",
    "**Strategy:**\n",
    "- Grid search over key hyperparameters\n",
    "- Track all metrics (HR@10, NDCG@10, MRR)\n",
    "- Save best configurations\n",
    "- Quick experiments (30-50 epochs with early stopping)\n",
    "\n",
    "**Time Estimate:** ~4-6 hours with GPU T4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267283ee",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d334666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in Kaggle or Colab\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If not already in project directory, clone it\n",
    "if not os.path.exists('length-adaptive'):\n",
    "    print(\"ðŸ“¦ Cloning repository...\")\n",
    "    !git clone https://github.com/faroukq1/length-adaptive.git\n",
    "    %cd length-adaptive\n",
    "else:\n",
    "    print(\"âœ… Repository already exists\")\n",
    "    if os.path.basename(os.getcwd()) != 'length-adaptive':\n",
    "        %cd length-adaptive\n",
    "\n",
    "print(f\"ðŸ“‚ Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a8be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"ðŸ“¥ Installing dependencies...\")\n",
    "!pip install -q torch-geometric tqdm scikit-learn pandas matplotlib seaborn\n",
    "\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db9f2c",
   "metadata": {},
   "source": [
    "## Step 2: Verify GPU and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfeb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"=\"*70)\n",
    "print(f\"ðŸš€ Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸ“Š GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ðŸ’¾ Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fedec3",
   "metadata": {},
   "source": [
    "## Step 3: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943fc701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project to path\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from src.data.dataloader import get_dataloaders\n",
    "\n",
    "print(\"ðŸ“‚ Loading MovieLens-1M data...\")\n",
    "\n",
    "# Load data - get_dataloaders returns all three loaders plus config\n",
    "train_loader, val_loader, test_loader, config = get_dataloaders(\n",
    "    data_path='data/ml-1m/processed/sequences.pkl',\n",
    "    batch_size=256,\n",
    "    max_len=200,\n",
    "    num_workers=2  # Reduce workers for Kaggle/Colab\n",
    ")\n",
    "\n",
    "# Get number of items from config\n",
    "num_items = config['num_items']\n",
    "\n",
    "print(f\"âœ… Data loaded successfully!\")\n",
    "print(f\"ðŸ“Š Number of items: {num_items}\")\n",
    "print(f\"ðŸ“Š Train batches: {len(train_loader)}\")\n",
    "print(f\"ðŸ“Š Validation batches: {len(val_loader)}\")\n",
    "print(f\"ðŸ“Š Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8790a6",
   "metadata": {},
   "source": [
    "## Step 4: Define Fine-Tuning Configuration\n",
    "\n",
    "**Hyperparameters to Tune:**\n",
    "1. Learning rate: [0.0005, 0.001, 0.002]\n",
    "2. d_model (embedding size): [64, 128]\n",
    "3. n_heads: [2, 4]\n",
    "4. n_blocks: [2, 3]\n",
    "5. Dropout: [0.1, 0.2]\n",
    "6. For discrete: L_short, L_long values\n",
    "7. For fixed: alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search space\n",
    "hyperparameter_configs = {\n",
    "    'bert_hybrid_fixed': [\n",
    "        # Original config\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.2, 'alpha': 0.5},\n",
    "        # Vary alpha\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.2, 'alpha': 0.3},\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.2, 'alpha': 0.7},\n",
    "        # Vary learning rate\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.0005, 'dropout': 0.2, 'alpha': 0.5},\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.002, 'dropout': 0.2, 'alpha': 0.5},\n",
    "        # Larger model\n",
    "        {'d_model': 128, 'n_heads': 4, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.2, 'alpha': 0.5},\n",
    "        # Deeper model\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 3, 'lr': 0.001, 'dropout': 0.2, 'alpha': 0.5},\n",
    "        # Lower dropout\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.1, 'alpha': 0.5},\n",
    "    ],\n",
    "    \n",
    "    'bert_hybrid_discrete': [\n",
    "        # Original config\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.2, 'L_short': 10, 'L_long': 30},\n",
    "        # Vary thresholds\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.2, 'L_short': 5, 'L_long': 20},\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.2, 'L_short': 15, 'L_long': 40},\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.2, 'L_short': 8, 'L_long': 25},\n",
    "        # Vary learning rate\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.0005, 'dropout': 0.2, 'L_short': 10, 'L_long': 30},\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.002, 'dropout': 0.2, 'L_short': 10, 'L_long': 30},\n",
    "        # Larger model\n",
    "        {'d_model': 128, 'n_heads': 4, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.2, 'L_short': 10, 'L_long': 30},\n",
    "        # Deeper model\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 3, 'lr': 0.001, 'dropout': 0.2, 'L_short': 10, 'L_long': 30},\n",
    "        # Lower dropout\n",
    "        {'d_model': 64, 'n_heads': 2, 'n_blocks': 2, 'lr': 0.001, 'dropout': 0.1, 'L_short': 10, 'L_long': 30},\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ”¬ Hyperparameter Search Space\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nbert_hybrid_fixed: {len(hyperparameter_configs['bert_hybrid_fixed'])} configurations\")\n",
    "print(f\"bert_hybrid_discrete: {len(hyperparameter_configs['bert_hybrid_discrete'])} configurations\")\n",
    "print(f\"\\nTotal experiments: {sum(len(v) for v in hyperparameter_configs.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87877e",
   "metadata": {},
   "source": [
    "## Step 5: Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.bert4rec_hybrid import HybridBERT4RecGNN\n",
    "from src.train.trainer import Trainer\n",
    "from src.train.loss import BPRLoss\n",
    "\n",
    "def create_model(model_type, num_items, config):\n",
    "    \"\"\"Create model with given configuration\"\"\"\n",
    "    fusion_type = model_type.replace('bert_hybrid_', '')\n",
    "    \n",
    "    # Base parameters\n",
    "    model_params = {\n",
    "        'num_items': num_items,\n",
    "        'd_model': config['d_model'],\n",
    "        'n_heads': config['n_heads'],\n",
    "        'n_blocks': config['n_blocks'],\n",
    "        'd_ff': config['d_model'] * 4,  # Standard transformer ratio\n",
    "        'max_len': 200,\n",
    "        'gnn_layers': 2,\n",
    "        'dropout': config['dropout'],\n",
    "        'fusion_type': fusion_type,\n",
    "    }\n",
    "    \n",
    "    # Add fusion-specific parameters\n",
    "    if fusion_type == 'fixed':\n",
    "        model_params['fixed_alpha'] = config['alpha']\n",
    "    elif fusion_type == 'discrete':\n",
    "        model_params['L_short'] = config['L_short']\n",
    "        model_params['L_long'] = config['L_long']\n",
    "    \n",
    "    return HybridBERT4RecGNN(**model_params)\n",
    "\n",
    "def train_model(model_type, config, epochs=50, patience=10):\n",
    "    \"\"\"Train a single model configuration\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ðŸš€ Training: {model_type}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Config: {config}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(model_type, num_items, config).to(device)\n",
    "    \n",
    "    # Setup training\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    criterion = BPRLoss()\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        max_epochs=epochs,\n",
    "        patience=patience,\n",
    "        eval_every=5\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = trainer.train()\n",
    "    \n",
    "    # Get best results\n",
    "    best_epoch = history['best_epoch']\n",
    "    best_metrics = history['val_metrics'][best_epoch]\n",
    "    \n",
    "    print(f\"\\nâœ… Training Complete!\")\n",
    "    print(f\"ðŸ“Š Best Epoch: {best_epoch}\")\n",
    "    print(f\"ðŸ“ˆ HR@10: {best_metrics['hr@10']:.6f}\")\n",
    "    print(f\"ðŸ“ˆ NDCG@10: {best_metrics['ndcg@10']:.6f}\")\n",
    "    print(f\"ðŸ“ˆ MRR: {best_metrics['mrr']:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'model_type': model_type,\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "        'best_metrics': best_metrics,\n",
    "        'best_epoch': best_epoch,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "print(\"âœ… Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e6a2e",
   "metadata": {},
   "source": [
    "## Step 6: Run Fine-Tuning Experiments\n",
    "\n",
    "This will train all configurations. Each run takes ~15-25 minutes with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea753aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path('results/finetuning')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ”¬ Starting Fine-Tuning Experiments\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ðŸ“ Results will be saved to: {results_dir}\")\n",
    "\n",
    "# Fine-tune bert_hybrid_fixed\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ¯ BERT Hybrid Fixed Models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, config in enumerate(hyperparameter_configs['bert_hybrid_fixed'], 1):\n",
    "    print(f\"\\nðŸ“ Configuration {i}/{len(hyperparameter_configs['bert_hybrid_fixed'])}\")\n",
    "    \n",
    "    try:\n",
    "        result = train_model('bert_hybrid_fixed', config, epochs=50, patience=10)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        result_file = results_dir / f'bert_hybrid_fixed_config{i}_{timestamp}.pkl'\n",
    "        with open(result_file, 'wb') as f:\n",
    "            pickle.dump(result, f)\n",
    "        \n",
    "        print(f\"ðŸ’¾ Saved to: {result_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in configuration {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Fine-tune bert_hybrid_discrete\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ¯ BERT Hybrid Discrete Models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, config in enumerate(hyperparameter_configs['bert_hybrid_discrete'], 1):\n",
    "    print(f\"\\nðŸ“ Configuration {i}/{len(hyperparameter_configs['bert_hybrid_discrete'])}\")\n",
    "    \n",
    "    try:\n",
    "        result = train_model('bert_hybrid_discrete', config, epochs=50, patience=10)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        result_file = results_dir / f'bert_hybrid_discrete_config{i}_{timestamp}.pkl'\n",
    "        with open(result_file, 'wb') as f:\n",
    "            pickle.dump(result, f)\n",
    "        \n",
    "        print(f\"ðŸ’¾ Saved to: {result_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in configuration {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… All Fine-Tuning Experiments Complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ðŸ“Š Total successful runs: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac144a2b",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aad319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_data = []\n",
    "\n",
    "for result in all_results:\n",
    "    row = {\n",
    "        'Model': result['model_type'],\n",
    "        'HR@10': result['best_metrics']['hr@10'],\n",
    "        'NDCG@10': result['best_metrics']['ndcg@10'],\n",
    "        'MRR': result['best_metrics']['mrr'],\n",
    "        'Best Epoch': result['best_epoch'],\n",
    "        **result['config']  # Add all config parameters\n",
    "    }\n",
    "    results_data.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Sort by NDCG@10\n",
    "results_df = results_df.sort_values('NDCG@10', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š Fine-Tuning Results Summary\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_csv = results_dir / 'finetuning_results.csv'\n",
    "results_df.to_csv(results_csv, index=False)\n",
    "print(f\"\\nðŸ’¾ Results saved to: {results_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cf045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best configurations\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ† Best Configurations\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_type in ['bert_hybrid_fixed', 'bert_hybrid_discrete']:\n",
    "    model_results = results_df[results_df['Model'] == model_type]\n",
    "    if len(model_results) > 0:\n",
    "        best_idx = model_results['NDCG@10'].idxmax()\n",
    "        best = model_results.loc[best_idx]\n",
    "        \n",
    "        print(f\"\\n{model_type}:\")\n",
    "        print(f\"  HR@10: {best['HR@10']:.6f}\")\n",
    "        print(f\"  NDCG@10: {best['NDCG@10']:.6f}\")\n",
    "        print(f\"  MRR: {best['MRR']:.6f}\")\n",
    "        print(f\"  Configuration:\")\n",
    "        for key, value in best.items():\n",
    "            if key not in ['Model', 'HR@10', 'NDCG@10', 'MRR', 'Best Epoch']:\n",
    "                print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd78c0b",
   "metadata": {},
   "source": [
    "## Step 8: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. NDCG@10 comparison\n",
    "ax = axes[0, 0]\n",
    "for model_type in ['bert_hybrid_fixed', 'bert_hybrid_discrete']:\n",
    "    model_data = results_df[results_df['Model'] == model_type]\n",
    "    ax.scatter(range(len(model_data)), model_data['NDCG@10'], \n",
    "              label=model_type, s=100, alpha=0.7)\n",
    "ax.set_xlabel('Configuration Index', fontsize=12)\n",
    "ax.set_ylabel('NDCG@10', fontsize=12)\n",
    "ax.set_title('NDCG@10 across Configurations', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. HR@10 vs NDCG@10\n",
    "ax = axes[0, 1]\n",
    "for model_type in ['bert_hybrid_fixed', 'bert_hybrid_discrete']:\n",
    "    model_data = results_df[results_df['Model'] == model_type]\n",
    "    ax.scatter(model_data['HR@10'], model_data['NDCG@10'], \n",
    "              label=model_type, s=100, alpha=0.7)\n",
    "ax.set_xlabel('HR@10', fontsize=12)\n",
    "ax.set_ylabel('NDCG@10', fontsize=12)\n",
    "ax.set_title('HR@10 vs NDCG@10', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Learning rate effect (for fixed)\n",
    "ax = axes[1, 0]\n",
    "if 'lr' in results_df.columns:\n",
    "    fixed_data = results_df[results_df['Model'] == 'bert_hybrid_fixed']\n",
    "    if len(fixed_data) > 0:\n",
    "        lr_groups = fixed_data.groupby('lr')['NDCG@10'].mean()\n",
    "        ax.bar(range(len(lr_groups)), lr_groups.values)\n",
    "        ax.set_xticks(range(len(lr_groups)))\n",
    "        ax.set_xticklabels([f'{lr:.4f}' for lr in lr_groups.index])\n",
    "        ax.set_xlabel('Learning Rate', fontsize=12)\n",
    "        ax.set_ylabel('Average NDCG@10', fontsize=12)\n",
    "        ax.set_title('Learning Rate Effect (Fixed)', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Model size effect\n",
    "ax = axes[1, 1]\n",
    "if 'd_model' in results_df.columns:\n",
    "    size_groups = results_df.groupby('d_model')['NDCG@10'].mean()\n",
    "    ax.bar(range(len(size_groups)), size_groups.values, color='coral')\n",
    "    ax.set_xticks(range(len(size_groups)))\n",
    "    ax.set_xticklabels([f'{size}' for size in size_groups.index])\n",
    "    ax.set_xlabel('Model Size (d_model)', fontsize=12)\n",
    "    ax.set_ylabel('Average NDCG@10', fontsize=12)\n",
    "    ax.set_title('Model Size Effect', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'finetuning_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualizations created and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f3e67",
   "metadata": {},
   "source": [
    "## Step 9: Test Best Models\n",
    "\n",
    "Evaluate the best configuration of each model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da50bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.metrics import evaluate_model\n",
    "\n",
    "# Evaluate best models on test set\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ¯ Testing Best Configurations\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for model_type in ['bert_hybrid_fixed', 'bert_hybrid_discrete']:\n",
    "    model_results = results_df[results_df['Model'] == model_type]\n",
    "    if len(model_results) == 0:\n",
    "        continue\n",
    "        \n",
    "    # Get best configuration\n",
    "    best_idx = model_results['NDCG@10'].idxmax()\n",
    "    best_result = [r for r in all_results if r['model_type'] == model_type][best_idx]\n",
    "    \n",
    "    print(f\"\\n{model_type}:\")\n",
    "    print(f\"  Best config: {best_result['config']}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model = best_result['model'].to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    test_metrics = evaluate_model(model, test_loader, device, k=10)\n",
    "    \n",
    "    print(f\"\\n  Test Results:\")\n",
    "    print(f\"    HR@10: {test_metrics['hr@10']:.6f}\")\n",
    "    print(f\"    NDCG@10: {test_metrics['ndcg@10']:.6f}\")\n",
    "    print(f\"    MRR: {test_metrics['mrr']:.6f}\")\n",
    "    \n",
    "    test_results.append({\n",
    "        'Model': model_type,\n",
    "        'Test_HR@10': test_metrics['hr@10'],\n",
    "        'Test_NDCG@10': test_metrics['ndcg@10'],\n",
    "        'Test_MRR': test_metrics['mrr'],\n",
    "        'Config': str(best_result['config'])\n",
    "    })\n",
    "\n",
    "# Save test results\n",
    "test_df = pd.DataFrame(test_results)\n",
    "test_csv = results_dir / 'test_results_best_configs.csv'\n",
    "test_df.to_csv(test_csv, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Test evaluation complete!\")\n",
    "print(\"=\"*70)\n",
    "print(test_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea81ae6a",
   "metadata": {},
   "source": [
    "## Step 10: Save Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df37111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best models\n",
    "models_dir = results_dir / 'best_models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ Saving Best Models\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_type in ['bert_hybrid_fixed', 'bert_hybrid_discrete']:\n",
    "    model_results = results_df[results_df['Model'] == model_type]\n",
    "    if len(model_results) == 0:\n",
    "        continue\n",
    "        \n",
    "    # Get best result\n",
    "    best_idx = model_results['NDCG@10'].idxmax()\n",
    "    best_result = [r for r in all_results if r['model_type'] == model_type][best_idx]\n",
    "    \n",
    "    # Save model\n",
    "    model_path = models_dir / f'{model_type}_best.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': best_result['model'].state_dict(),\n",
    "        'config': best_result['config'],\n",
    "        'metrics': best_result['best_metrics'],\n",
    "        'epoch': best_result['best_epoch']\n",
    "    }, model_path)\n",
    "    \n",
    "    print(f\"âœ… Saved {model_type} to {model_path}\")\n",
    "    print(f\"   NDCG@10: {best_result['best_metrics']['ndcg@10']:.6f}\")\n",
    "\n",
    "print(\"\\nðŸ’¾ All models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe52f45c",
   "metadata": {},
   "source": [
    "## Step 11: Download Results\n",
    "\n",
    "Package all results for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece36078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create zip file\n",
    "print(\"ðŸ“¦ Creating results package...\")\n",
    "\n",
    "zip_path = '/tmp/finetuning_results'\n",
    "shutil.make_archive(zip_path, 'zip', results_dir)\n",
    "\n",
    "print(f\"âœ… Results packaged: {zip_path}.zip\")\n",
    "print(\"\\nðŸ“¥ Download the file to get all results!\")\n",
    "print(f\"\\nIncluded:\")\n",
    "print(f\"  - Individual experiment results (pkl files)\")\n",
    "print(f\"  - Summary CSV: finetuning_results.csv\")\n",
    "print(f\"  - Test results: test_results_best_configs.csv\")\n",
    "print(f\"  - Visualizations: finetuning_analysis.png\")\n",
    "print(f\"  - Best models: best_models/*.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a000a5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Fine-Tuning Complete! ðŸŽ‰**\n",
    "\n",
    "This notebook has:\n",
    "1. âœ… Trained multiple configurations of bert_hybrid_fixed and bert_hybrid_discrete\n",
    "2. âœ… Tested variations in: learning rate, model size, depth, dropout, fusion parameters\n",
    "3. âœ… Identified best configurations for each model\n",
    "4. âœ… Evaluated best models on test set\n",
    "5. âœ… Saved all results and best model checkpoints\n",
    "\n",
    "**Next Steps:**\n",
    "- Review the results CSV to see all configurations\n",
    "- Check the visualizations for insights\n",
    "- Use the best models for production or further experiments\n",
    "- Consider testing on other datasets"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
